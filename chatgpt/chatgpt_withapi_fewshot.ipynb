{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7fa3ee9-62ba-4849-9f83-8584139e295f",
   "metadata": {},
   "source": [
    "## ChatGPT with Few Shot\n",
    "\n",
    "Include some examples for ChatGPT of histograms.\n",
    "\n",
    "**NOTE:** this is left here for reference, but not used in the paper (lack of improvement in results AND many other works indicating the lack of improvement as well for fewshot and images generally)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6baf77f-b887-437d-b0c6-20cde719d906",
   "metadata": {},
   "source": [
    "Docs:\n",
    "\n",
    "* how to upload images: https://platform.openai.com/docs/guides/vision\n",
    "* types of models: https://stackoverflow.com/questions/75774873/openai-api-error-this-is-a-chat-model-and-not-supported-in-the-v1-completions\n",
    "* token limits: https://platform.openai.com/settings/organization/limits\n",
    "* models: https://platform.openai.com/docs/models/model-endpoint-compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25dda74-0b62-4a27-9566-aaaa27727c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017cce17-f4e8-4df6-a3b5-2edef3714601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir_api = '~/Dropbox/Paper_JCDL2025/chatgpt_api/' # where to store API results\n",
    "\n",
    "\n",
    "key_file = '~/.openai/key.txt'\n",
    "\n",
    "# for fewshot\n",
    "dir_api = '~/LLM_VQA_JCDL2025/example_hists/LLM_outputs/chatgpt_api_fewshot/' #store API results for example-hists\n",
    "fewshot_dir = '~/LLM_VQA_JCDL2025/example_hists/fewshot/' # assume images in \"imgs\", jsons in \"jsons\"\n",
    "fac_fewshot = 1.0\n",
    "\n",
    "# for fewshot, more examples\n",
    "dir_api = '~/LLM_VQA_JCDL2025/example_hists/LLM_outputs/chatgpt_api_fewshot2/' #store API results for example-hists\n",
    "fewshot_dir = '~/LLM_VQA_JCDL2025/example_hists/fewshot2/' # assume images in \"imgs\", jsons in \"jsons\"\n",
    "fac_fewshot = 0.5\n",
    "\n",
    "# for testing images\n",
    "jsons_dir = '~/LLM_VQA_JCDL2025/example_hists/jsons/' # directory where jsons created with figure are stored\n",
    "imgs_dir = '~/LLM_VQA_JCDL2025/example_hists/imgs/' # where images are stored\n",
    "\n",
    "# for saving temp images for reading in\n",
    "tmp_dir = '~/Downloads/tmp/'\n",
    "\n",
    "img_format = 'jpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf577c-4a80-4fbf-b4a0-8a0a61d16b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from glob import glob\n",
    "from copy import deepcopy\n",
    "\n",
    "# debug\n",
    "from importlib import reload\n",
    "\n",
    "from sys import path\n",
    "path.append('../')\n",
    "import utils.llm_utils\n",
    "reload(utils.llm_utils)\n",
    "from utils.llm_utils import parse_qa, load_image, get_img_json_pair, parse_for_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddbfe28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_question_and_prompt(question_list, encoded_image, verbose = True):\n",
    "    # current question format is: ['persona', 'context','question', 'format'] (see readme in example_hists)\n",
    "    question = question_list['context'] + \" \" + question_list['question'] + \" \" + question_list['format']\n",
    "    # lowercase the first word, just in case\n",
    "    question = question.lstrip() # no whitespace\n",
    "    question = question[0].lower() + question[1:]\n",
    "    if verbose: print('   on question:',question)\n",
    "    # Prepare the API request\n",
    "    #prompt = f\"I am going to show you an image. Here is the image: [Image: {encoded_image}]. Now, {question}\"\n",
    "    #prompt_save = f\"I am going to show you an image. Here is the image: [Image: <ENCODED IMAGE>]. Now, {question}\"\n",
    "    prompt = f\"I am going to show you an image. Now, {question}\"\n",
    "    prompt_save = f\"I am going to show you an image. Now, {question}\"\n",
    "    return prompt, prompt_save, question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855459bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fewshot(fewshot_dir, question_list, \n",
    "                   tmp_dir = '~/Downloads/tmp/',\n",
    "                   img_dir = 'imgs', json_dir = 'jsons',\n",
    "                   fac = 1.0, restart=False, verbose=False, \n",
    "                    types = ['(words + list)', '(words)']\n",
    "):\n",
    "    # get question\n",
    "    #question = \n",
    "    prompt = 'Here are some example questions with figures and their respective answers. Reference examples delimited with “”\"\\n'\n",
    "    prompt += '\"\"\"\\n'\n",
    "    prompt_dummy = deepcopy(prompt)\n",
    "    json_files = glob(fewshot_dir + '/' + json_dir + '/*')\n",
    "    # print(fewshot_dir + '/' + json_dir + '/*')\n",
    "    # print(json_files)\n",
    "    if verbose:\n",
    "        print('MAIN QUESTION:', question_list['Q'])\n",
    "    for json_path in json_files:\n",
    "        if verbose: \n",
    "            print('-------- JSON: ' + json_path, '-----------------')\n",
    "        img_path = imgs_dir + json_path.split('/')[-1].removesuffix('.json') + '.' + img_format\n",
    "        encoded_image, img_format_media, base_json, err = get_img_json_pair(img_path, json_path, None, \n",
    "                                                            fac=fac, restart=restart,\n",
    "                                                            tmp_dir=tmp_dir)\n",
    "        \n",
    "        if err:\n",
    "            print('[ERROR]: error in create_fewshot')\n",
    "            print('json_path:', json_path)\n",
    "            print('dir_api:', dir_api)\n",
    "            continue\n",
    "        # response, prompt_dummy_full = send_to_chatgpt(question_list, client, img_path, encoded_image,\n",
    "        #             model = model, img_format = img_format_media,\n",
    "        #             test_run = True, subset_questions_by_keys=subset_questions_by_keys)\n",
    "        _, prompt_dummy_full, _ =  make_question_and_prompt(question_list, encoded_image, verbose = verbose)\n",
    "\n",
    "        #**HERE** response['a'] needs to be from each image loaded here's json!\n",
    "        # level_parse = 'Level 3'\n",
    "        # plot_level = 'Plot-level questions'\n",
    "        # qa = parse_qa(level_parse, plot_level, qa, base_json['VQA'], types)\n",
    "        qa = []\n",
    "        level_parse = question_list['Level']\n",
    "        plot_level = question_list['type']\n",
    "        try:\n",
    "            qa = parse_qa(level_parse, plot_level, qa, base_json['VQA'], types)\n",
    "        except Exception as e:\n",
    "            print('[ERROR]:', str(e))\n",
    "            print('base json is:', base_json)\n",
    "        # pick the right question to get the right answer\n",
    "        answer = None\n",
    "        for qa_pair in qa:\n",
    "            if qa_pair['Q'] == question_list['Q']:\n",
    "                answer = qa_pair['A']\n",
    "        if answer is None:\n",
    "            print(\"[ERROR]: question not found\")\n",
    "            print('qa:')\n",
    "            print(qa)\n",
    "            print('----------')\n",
    "            print('question list:', question_list)\n",
    "            print('****************')\n",
    "            import sys; sys.exit()\n",
    "        elif verbose:\n",
    "            print('Answer:', answer)\n",
    "\n",
    "        prompt_dummy += 'QUESTION: ' + prompt_dummy_full + '\\n'\n",
    "        prompt_dummy += 'ANSWER: ' + str(answer) + '\\n'\n",
    "        prompt_dummy += '\"\"\"\\n'\n",
    "        # replace text\n",
    "        replace_text = f\"{encoded_image}\"\n",
    "        prompt += 'QUESTION: ' + prompt_dummy_full.replace('[Image: <ENCODED IMAGE>]', replace_text) + '\\n'\n",
    "        prompt += 'ANSWER: ' + str(answer) + '\\n'\n",
    "        prompt += '\"\"\"\\n'\n",
    "\n",
    "    # stuff\n",
    "    #prompt += '\"\"\"'\n",
    "    return prompt, prompt_dummy #, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834a47f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "907cc1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test one\n",
    "fac = 1.0\n",
    "question_list_test = {'Q': 'You are a helpful assistant that can analyze images.  How many gaussians were used to generate the data for the plot in the figure panel? Please format the output as a json as {\"ngaussians\":\"\"} for this figure panel, where the \"ngaussians\" value should be an integer.',\n",
    " 'A': 1,\n",
    " 'Level': 'Level 3',\n",
    " 'type': 'Plot-level questions',\n",
    " 'Response': 'TEST RUN',\n",
    " 'persona': 'You are a helpful assistant that can analyze images.',\n",
    " 'context': '',\n",
    " 'question': 'How many gaussians were used to generate the data for the plot in the figure panel? Please choose an integer number from 1 to 5.',\n",
    " 'format': 'Please format the output as a json as {\"ngaussians\":\"\"} for this figure panel, where the \"ngaussians\" value should be an integer.',\n",
    " 'plot number': 'plot0',\n",
    " 'Response String': 'TEST RUN',\n",
    " 'prompt': 'I am going to show you an image. Here is the image: [Image: <ENCODED IMAGE>]. Now, how many gaussians were used to generate the data for the plot in the figure panel? Please choose an integer number from 1 to 5. Please format the output as a json as {\"ngaussians\":\"\"} for this figure panel, where the \"ngaussians\" value should be an integer.',\n",
    " 'extra sys prompt': ''\n",
    " }\n",
    "prompt_sys, prompt_dummy_sys = create_fewshot(fewshot_dir, \n",
    "                                                question_list_test, \n",
    "                                                tmp_dir = tmp_dir,\n",
    "                                                fac = fac, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a759a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(prompt_dummy_sys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f84324",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d2d69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_chatgpt(question_list, client, image_path, encoded_image,\n",
    "                    model =\"gpt-4o-mini\", \n",
    "                    tmp_dir = '~/Downloads/tmp/',\n",
    "                    test_run = True, fac=1.0, img_format='png',\n",
    "                    verbose=True, \n",
    "                    subset_questions_by_keys = None,\n",
    "                    use_fewshot = False,\n",
    "                    types = ['(words + list)', '(words)']\n",
    "                        ):\n",
    "    # models as of ~May 2024\n",
    "    #model=\"gpt-4\",\n",
    "    #model=\"gpt-4o\",\n",
    "    #model =\"gpt-4o-mini\",\n",
    "    #model =\"gpt-3.5-turbo\",\n",
    "    #model=\"gpt-3.5-turbo-instruct\",\n",
    "    \"\"\"\n",
    "    subset_questions_by_keys : only use a subset of questions to ping to model, e.g. ['median', 'how many gaussians']\n",
    "    use_fewshot : if True will use some examples from the fewshot directory\n",
    "    \"\"\"\n",
    "\n",
    "    iFac = 1.0 # just in case we want to progressively make the image smaller\n",
    "    success = False\n",
    "    is_subset = False\n",
    "    prompt_dummy_sys = ''\n",
    "    while not success:\n",
    "        try:\n",
    "            # # current question format is: ['persona', 'context','question', 'format'] (see readme in example_hists)\n",
    "            # question = question_list['context'] + \" \" + question_list['question'] + \" \" + question_list['format']\n",
    "            # # lowercase the first word, just in case\n",
    "            # question = question.lstrip() # no whitespace\n",
    "            # question = question[0].lower() + question[1:]\n",
    "            # if verbose: print('   on question:',question)\n",
    "            # # Prepare the API request\n",
    "            # prompt = f\"I am going to show you an image. Here is the image: [Image: {encoded_image}]. Now, {question}\"\n",
    "            # prompt_save = f\"I am going to show you an image. Here is the image: [Image: <ENCODED IMAGE>]. Now, {question}\"\n",
    "            prompt, prompt_save, question =  make_question_and_prompt(question_list, encoded_image, verbose = verbose)\n",
    "\n",
    "            if subset_questions_by_keys is not None and type(subset_questions_by_keys) == type([]): # make sure list\n",
    "                for s in subset_questions_by_keys:\n",
    "                    if s in question:\n",
    "                        is_subset = True\n",
    "            else:\n",
    "                is_subset = True\n",
    "            \n",
    "            if not test_run and is_subset:\n",
    "                if use_fewshot:\n",
    "                    #print(\"HI THERE\")\n",
    "                    prompt_sys, prompt_dummy_sys = create_fewshot(fewshot_dir, \n",
    "                                                                 question_list, \n",
    "                                                                 tmp_dir = tmp_dir,\n",
    "                                                                 fac = fac, types=types)\n",
    "                    #print(\"FAC IS:\", fac)\n",
    "                else:\n",
    "                    prompt_sys = ''; prompt_dummy_sys = ''\n",
    "                # Send the request to the GPT-4o API\n",
    "                response = client.chat.completions.create(\n",
    "                    model = model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": question_list['persona'] + ' ' + prompt_sys},\n",
    "                        {\"role\":\"user\", \"content\": [\n",
    "                            {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt\n",
    "                            },\n",
    "                            {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                #\"url\": f\"data:image/jpeg;base64,{encoded_image}\" # PNG encoding??? does thsi need to change??\n",
    "                                \"url\": f\"data:image/{img_format};base64,{encoded_image}\" \n",
    "                            }\n",
    "                            }\n",
    "                        ]\n",
    "                        }\n",
    "                    ]\n",
    "                )\n",
    "                success = True\n",
    "            elif not is_subset:\n",
    "                response = 'Not asked'\n",
    "                success = True\n",
    "            elif use_fewshot:\n",
    "                prompt_sys, prompt_dummy_sys = create_fewshot(fewshot_dir, \n",
    "                                                                question_list, \n",
    "                                                                tmp_dir = tmp_dir,\n",
    "                                                                fac = fac, types=types)\n",
    "                success = True\n",
    "            else:\n",
    "                success = True\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            new_fac = fac/iFac\n",
    "            print('new fac = ', new_fac)\n",
    "            encoded_image = load_image(image_path,fac=new_fac, tmp_dir=tmp_dir)\n",
    "            iFac += 1\n",
    "            #import sys; sys.exit()\n",
    "            #success = True\n",
    "            # break\n",
    "            #klaksljf\n",
    "    \n",
    "    if not test_run and is_subset:\n",
    "        # Get the response from the API\n",
    "        answer = response.choices[0].message.content\n",
    "        question_list['raw answer'] = answer\n",
    "        # format answer\n",
    "        answer_format = answer.replace(\"```json\\n\",'').replace(\"\\n```\",'')\n",
    "        try:\n",
    "            question_list['Response'] = json.loads(answer_format)\n",
    "        except:\n",
    "            question_list['Response'] = answer_format\n",
    "            question_list['Error'] = 'JSON formatting'\n",
    "        question_list['Response String'] = answer_format\n",
    "        success = True\n",
    "    elif not is_subset:\n",
    "        answer = response\n",
    "        question_list['raw answer'] = answer\n",
    "        question_list['Response'] = answer\n",
    "        question_list['Response String'] = answer\n",
    "        success = True\n",
    "    else:\n",
    "        question_list['Response'] = 'TEST RUN'\n",
    "        question_list['Response String'] = 'TEST RUN'\n",
    "\n",
    "    return question_list, prompt_save, prompt_dummy_sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04dab3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_qa(pickle_file, qa_in, subset_questions_by_keys=None, showNotAsked=False):\n",
    "    if subset_questions_by_keys is not None:\n",
    "        print('---------- ASKED ----------')\n",
    "    print(pickle_file)\n",
    "    print('*********')\n",
    "    for qa_pairs in qa_in:\n",
    "        hasSub = False\n",
    "        if subset_questions_by_keys is not None and type(subset_questions_by_keys) == type([]):\n",
    "            for s in subset_questions_by_keys:\n",
    "                if s in qa_pairs['prompt']:\n",
    "                    hasSub = True\n",
    "        else:\n",
    "            hasSub = True\n",
    "\n",
    "        if hasSub:\n",
    "            print('Prompt:', qa_pairs['prompt'])\n",
    "            print('   Real A:', qa_pairs['A'])\n",
    "            print('ChatGPT A:', qa_pairs['Response'])\n",
    "            print('')\n",
    "\n",
    "    if subset_questions_by_keys is not None and showNotAsked:\n",
    "        print('')\n",
    "        print('')\n",
    "        print('------------ NOT ASKED -----------')\n",
    "        for qa_pairs in qa_in:\n",
    "            hasSub = False\n",
    "            if subset_questions_by_keys is not None and type(subset_questions_by_keys) == type([]):\n",
    "                for s in subset_questions_by_keys:\n",
    "                    if s in qa_pairs['prompt']:\n",
    "                        hasSub = True\n",
    "\n",
    "            if not hasSub:\n",
    "                print('Prompt:', qa_pairs['prompt'])\n",
    "                print('   Real A:', qa_pairs['A'])\n",
    "                print('ChatGPT A:', qa_pairs['Response'])\n",
    "                print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb280d-d109-4a03-8a76-ea47d5f8960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "with open(key_file,'r') as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=api_key.strip(),  # this is also the default, it can be omitted\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8957d21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "jsons_to_parse = glob(jsons_dir + '/*.json')\n",
    "jsons_to_parse[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8382b9",
   "metadata": {},
   "source": [
    "Look at a possible questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e019153",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path = jsons_to_parse[0]\n",
    "verbose = False\n",
    "restart = False\n",
    "\n",
    "fac = 0.5\n",
    "\n",
    "img_path = imgs_dir + json_path.split('/')[-1].removesuffix('.json') + '.' + img_format\n",
    "encoded_image, img_format_media, base_json, err = get_img_json_pair(img_path, json_path, None, \n",
    "                                                    fac=fac, restart=restart,\n",
    "                                                    tmp_dir=tmp_dir)\n",
    "\n",
    "base_json['VQA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d60b9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc7883",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = \"gpt-5-nano-2025-08-07\"\n",
    "#subset_questions_by_keys = ['median', 'ngaussians'] # set to None to do all questions\n",
    "\n",
    "#p, pf, response = create_fewshot(fewshot_dir, dir_api, question_list, model, subset_questions_by_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03fccfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(pf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26b02ba9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62da723",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb0608b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iMax = 150 # should be 80\n",
    "verbose = False\n",
    "test_run = False # run w/o actually pinging openai\n",
    "restart = False\n",
    "print_sysprompt = False\n",
    "use_fewshot = True\n",
    "#model =\"gpt-4o-mini\"\n",
    "model = \"gpt-5-nano-2025-08-07\"\n",
    "\n",
    "subset_questions_by_keys = ['median', 'ngaussians'] # set to None to do all questions\n",
    "\n",
    "fac = fac_fewshot\n",
    "for ijson,json_path in enumerate(jsons_to_parse):\n",
    "    if ijson >= iMax:\n",
    "        continue\n",
    "\n",
    "    print('on', ijson+1, 'of', min(iMax,len(jsons_to_parse)), ':', json_path.split('/')[-1])\n",
    "\n",
    "    # get image and base json\n",
    "    img_path = imgs_dir + json_path.split('/')[-1].removesuffix('.json') + '.' + img_format\n",
    "    encoded_image, img_format_media, base_json, err = get_img_json_pair(img_path, json_path, dir_api, \n",
    "                                                      fac=fac, restart=restart,\n",
    "                                                      tmp_dir=tmp_dir)\n",
    "\n",
    "    if err:\n",
    "        continue\n",
    "\n",
    "\n",
    "    ###### create QA ########\n",
    "    qa = []\n",
    "    \n",
    "    for k,v in base_json['VQA']['Level 1']['Figure-level questions'].items():\n",
    "        out = {'Q':v['Q'], 'A':v['A'], 'Level':'Level 1', 'type':'Figure-level questions', 'Response':\"\"}\n",
    "        qa.append(out)\n",
    "    \n",
    "    # what kinds?\n",
    "    types = ['(words + list)', '(words)']\n",
    "    \n",
    "    # get uniques\n",
    "    level_parse = 'Level 1'\n",
    "    plot_level = 'Plot-level questions'\n",
    "    qa = parse_qa(level_parse, plot_level, qa, base_json['VQA'], types)\n",
    "    \n",
    "    level_parse = 'Level 2'\n",
    "    plot_level = 'Plot-level questions'\n",
    "    qa = parse_qa(level_parse, plot_level, qa, base_json['VQA'], types)\n",
    "    \n",
    "    level_parse = 'Level 3'\n",
    "    plot_level = 'Plot-level questions'\n",
    "    qa = parse_qa(level_parse, plot_level, qa, base_json['VQA'], types)\n",
    "\n",
    "    responses = []\n",
    "    for question_list in qa:\n",
    "        response, prompt, prompt_sys = send_to_chatgpt(question_list, client, img_path, encoded_image,\n",
    "                    model = model, img_format = img_format_media,\n",
    "                    test_run = test_run, subset_questions_by_keys=subset_questions_by_keys, \n",
    "                    types=types, use_fewshot=use_fewshot, fac=fac)\n",
    "        responses.append(response)\n",
    "        question_list['prompt'] = prompt\n",
    "        question_list['extra sys prompt'] = prompt_sys\n",
    "        if print_sysprompt:\n",
    "            print('       System prompt:', prompt_sys)\n",
    "\n",
    "    # parse for errors\n",
    "    qa = parse_for_errors(qa)\n",
    "\n",
    "    # dump to file\n",
    "    if not test_run:\n",
    "        with open(dir_api + json_path.split('/')[-1].removesuffix('.json')+ '.pickle', 'wb') as ff:\n",
    "            pickle.dump([qa, model], ff)\n",
    "        print(\"just saved:\", dir_api + json_path.split('/')[-1].removesuffix('.json')+ '.pickle')\n",
    "    else:\n",
    "        print('Would store at:', dir_api + json_path.split('/')[-1].removesuffix('.json')+ '.pickle')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55263b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fac_fewshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83c4b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042a456-e8cf-484f-958c-75cb9b0f9606",
   "metadata": {},
   "source": [
    "## Look at data\n",
    "\n",
    "Check out one, if you wanna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6974b8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickles = glob(dir_api + '*.pickle')\n",
    "pickles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636df058",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifile = 0\n",
    "with open(pickles[ifile], 'rb') as f:\n",
    "    qa_in = pickle.load(f)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11948f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_qa(pickles[ifile], qa_in, subset_questions_by_keys=subset_questions_by_keys, showNotAsked=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d309dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JCDL2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

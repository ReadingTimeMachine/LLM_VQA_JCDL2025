{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7fa3ee9-62ba-4849-9f83-8584139e295f",
   "metadata": {},
   "source": [
    "## ChatGPT\n",
    "\n",
    "This is just some messing around."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6baf77f-b887-437d-b0c6-20cde719d906",
   "metadata": {},
   "source": [
    "Docs:\n",
    "\n",
    "* how to upload images: https://platform.openai.com/docs/guides/vision\n",
    "* types of models: https://stackoverflow.com/questions/75774873/openai-api-error-this-is-a-chat-model-and-not-supported-in-the-v1-completions\n",
    "* token limits: https://platform.openai.com/settings/organization/limits\n",
    "* models: https://platform.openai.com/docs/models/model-endpoint-compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f25dda74-0b62-4a27-9566-aaaa27727c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017cce17-f4e8-4df6-a3b5-2edef3714601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir_api = '/Users/jnaiman/Dropbox/Paper_JCDL2025/chatgpt_api/' # where to store API results\n",
    "\n",
    "dir_api = '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/LLM_outputs/chatgpt_api/' #store API results for example-hists\n",
    "\n",
    "key_file = '/Users/jnaiman/.openai/key.txt'\n",
    "\n",
    "jsons_dir = '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/jsons/' # directory where jsons created with figure are stored\n",
    "imgs_dir = '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/imgs/' # where images are stored\n",
    "\n",
    "# for saving temp images for reading in\n",
    "tmp_dir = '/Users/jnaiman/Downloads/tmp/'\n",
    "\n",
    "img_format = 'jpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cf577c-4a80-4fbf-b4a0-8a0a61d16b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from sys import path\n",
    "path.append('../')\n",
    "from utils.llm_utils import parse_qa, load_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc26f67-1869-44f7-97bb-238b2c5a42b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_image(image_path, fac=0.5):\n",
    "#     img = Image.open(image_path).convert('RGB')\n",
    "#     new_size = np.round(np.array(img.size)*fac).astype('int')\n",
    "#     img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "#     #img = np.array(img)\n",
    "#     #with open(image_path, \"rb\") as image_file:\n",
    "#     img.save('/Users/jnaiman/Downloads/tmp/tmp_img.png')\n",
    "#     with open('/Users/jnaiman/Downloads/tmp/tmp_img.png','rb') as image_file:\n",
    "#         #return base64.b64encode(img).decode(\"utf-8\")\n",
    "#         return base64.b64encode(image_file.read()).decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe835962-2db1-4c56-bdf4-50708fedaad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parsing\n",
    "# def parse_qa(level_parse, plot_level, qa, j, types, \n",
    "#              partials = ['persona', 'context','question', 'format']):\n",
    "#     keys_tmp = list(j[level_parse][plot_level].keys())\n",
    "#     keys = []\n",
    "#     for k in keys_tmp:\n",
    "#         if '(' in k:\n",
    "#             k = k.split('(')[0].rstrip()\n",
    "#         keys.append(k)\n",
    "    \n",
    "#     keys = np.unique(keys).tolist()\n",
    "\n",
    "#     dirs_partials = {}\n",
    "    \n",
    "#     for k in keys:\n",
    "#         v = ''\n",
    "#         kk = ''\n",
    "#         for t in types:\n",
    "#             if k + \" \" + t in j[level_parse][plot_level]:\n",
    "#                 v = j[level_parse][plot_level][k + \" \" + t]\n",
    "#                 #kk = k + \" \" + t\n",
    "#                 break\n",
    "#         if v == '':\n",
    "#             v = j[level_parse][plot_level][k]\n",
    "#             #kk = k\n",
    "#         if 'A' in v: # no plot\n",
    "#             if type(v['A']) == type({}):\n",
    "#                 ans = list(v['A'].values())[0]\n",
    "#             else:\n",
    "#                 ans = v['A']\n",
    "#             if type(v['Q']) == type({}):\n",
    "#                 que = list(v['Q'].values())[0]\n",
    "#             else:\n",
    "#                 que = v['Q']\n",
    "#             # get other elements\n",
    "#             for p in partials:\n",
    "#                 dirs_partials[p] = v[p]\n",
    "#         else: # plotX\n",
    "#             for kk,vv in v.items():\n",
    "#                 ans = vv['A']\n",
    "#                 while type(ans) == type({}):\n",
    "#                     ans_1 = list(ans.values())[0]\n",
    "#                     if 'plot' in list(ans.keys())[0]:\n",
    "#                         ans = ans_1\n",
    "#                         break\n",
    "#                     ans = ans_1\n",
    "#                 que = vv['Q']\n",
    "#                 # get other elements\n",
    "#                 for p in partials:\n",
    "#                     dirs_partials[p] = vv[p]\n",
    "#         out = {'Q':que, 'A':ans, 'Level':level_parse, 'type':plot_level, 'Response':\"\"}\n",
    "#         for kp,vp in dirs_partials.items():\n",
    "#             out[kp] = vp\n",
    "#         # if there is a plot\n",
    "#         if 'plot' in list(v.keys())[0]:\n",
    "#             out['plot number'] = list(v.keys())[0]\n",
    "#             #import sys; sys.exit()\n",
    "#         qa.append(out)\n",
    "#     return qa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f95b5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_json_pair(img_path, json_path, dir_api, \n",
    "                      tmp_dir = '/Users/jnaiman/Downloads/tmp/',\n",
    "                      fac = 1.0, \n",
    "                      restart = False, verbose = True):\n",
    "    \"\"\"\n",
    "    img_path : where image file is stored\n",
    "    json_path : where json path is stored\n",
    "    dir_api : where we can look for prior, saved pickles, if applicable\n",
    "    fac : do we want to downsize the image? IF so, set to < 1\n",
    "\n",
    "    returns: encoded image, full json from creation run, error\n",
    "      encoded image and full json are empty strings if error is True\n",
    "    \"\"\"\n",
    "    #print('on', iFile, 'of', iMax)\n",
    "    err = False\n",
    "    base_file = json_path.split('/')[-1].removesuffix('.json')\n",
    "    if os.path.exists(dir_api + base_file + '.pickle') and not restart:\n",
    "        if verbose: print('have file already:', dir_api + base_file + '.pickle')\n",
    "        return '','', True\n",
    "    # do we have it?\n",
    "    try:\n",
    "        #image_path = '/Users/jnaiman/Downloads/data_full_v2/Picture'+str(int(iFile))+'.png'\n",
    "        encoded_image = load_image(img_path, fac=fac, tmp_dir=tmp_dir)\n",
    "        # get questions\n",
    "        with open(json_path,'r') as f:\n",
    "            j = json.loads(f.read())\n",
    "            j = json.loads(j)\n",
    "    except Exception as e:\n",
    "        if verbose: print('[ERROR]:', str(e))\n",
    "        err = True\n",
    "        return '','', True\n",
    "    \n",
    "    return encoded_image, j, err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d2d69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_chatgpt(question_list, client, image_path, encoded_image,\n",
    "                    model =\"gpt-4o-mini\", \n",
    "                    test_run = True, fac=1.0, \n",
    "                    verbose=True):\n",
    "    #model=\"gpt-4\",\n",
    "    #model=\"gpt-4o\",\n",
    "    #model =\"gpt-4o-mini\",\n",
    "    #model =\"gpt-3.5-turbo\",\n",
    "    #model=\"gpt-3.5-turbo-instruct\",\n",
    "\n",
    "    # query chatgpt\n",
    "    #responses = []\n",
    "\n",
    "    #try:\n",
    "    iFac = 1.0\n",
    "    #for question_list in qa:\n",
    "    success = False\n",
    "    iFac = 1\n",
    "    #['persona', 'context','question', 'format']\n",
    "    while not success:\n",
    "        try:\n",
    "            #question = question_list['Q']\n",
    "            question = question_list['context'] + \" \" + question_list['question'] + \" \" + question_list['format']\n",
    "            # lowercase the first word, just in case\n",
    "            question = question.lstrip() # no whitespace\n",
    "            question = question[0].lower() + question[1:]\n",
    "            if verbose: print('   on question:',question)\n",
    "            # Prepare the API request\n",
    "            prompt = f\"I am going to show you an image. Here is the image: [Image: {encoded_image}]. Now, {question}\"\n",
    "            prompt_save = f\"I am going to show you an image. Here is the image: [Image: <ENCODED IMAGE>]. Now, {question}\"\n",
    "            ##question_list['prompt'] = prompt\n",
    "            \n",
    "            if not test_run:\n",
    "                # Send the request to the GPT-4o API\n",
    "                response = client.chat.completions.create(\n",
    "                    model = model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": question_list['persona']},\n",
    "                        {\"role\":\"user\", \"content\": [\n",
    "                            {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt\n",
    "                            },\n",
    "                            {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "                            }\n",
    "                            }\n",
    "                        ]\n",
    "                        }\n",
    "                    ]\n",
    "                )\n",
    "                success = True\n",
    "            else:\n",
    "                success = True\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            new_fac = fac/iFac\n",
    "            print('new fac = ', new_fac)\n",
    "            encoded_image = load_image(image_path,fac=new_fac, tmp_dir=tmp_dir)\n",
    "            iFac += 1\n",
    "    \n",
    "    if not test_run:\n",
    "        # Get the response from the API\n",
    "        answer = response.choices[0].message.content\n",
    "        # format answer\n",
    "        answer_format = answer.replace(\"```json\\n\",'').replace(\"\\n```\",'')\n",
    "        try:\n",
    "            question_list['Response'] = json.loads(answer_format)\n",
    "        except:\n",
    "            question_list['Response'] = answer_format\n",
    "            question_list['Error'] = 'JSON formatting'\n",
    "        question_list['Response String'] = answer_format\n",
    "        success = True\n",
    "    else:\n",
    "        question_list['Response'] = 'TEST RUN'\n",
    "        question_list['Response String'] = 'TEST RUN'\n",
    "\n",
    "    return question_list, prompt_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b3cc147f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_for_errors(qa, verbose=True, print_what = 'prompt'):\n",
    "    \"\"\"\n",
    "    print_what : set to \"prompt\" to print full prompt, or \"question\" for just the question\n",
    "    \"\"\"\n",
    "    # try to fix\n",
    "    for level in qa:\n",
    "        noErr = False\n",
    "        if 'Error' in level:\n",
    "            noErr = True\n",
    "            try:\n",
    "                iters = []\n",
    "                for x in re.finditer(r'//(\\s*)(.*)(\\s*)\\n', level['Response']):\n",
    "                    #print(x)\n",
    "                    iters.append(level['Response'][x.span()[0]:x.span()[1]])\n",
    "                \n",
    "                response = level['Response']\n",
    "                for i in iters:\n",
    "                    response = response.replace(i,'')\n",
    "                level['Response'] = response\n",
    "                del level['Error']\n",
    "            except:\n",
    "                noErr = False\n",
    "                print('********')\n",
    "                print('couldnt fix:', level['Response'])\n",
    "                print('********')\n",
    "                pass\n",
    "    \n",
    "        try:\n",
    "            if type(level['Response']) != type({}):\n",
    "                level['Response'] = json.loads(level['Response'])\n",
    "        except:\n",
    "            pass\n",
    "    \n",
    "        if not noErr and verbose:\n",
    "            if print_what == 'question':\n",
    "                print('Q:', level['Q'])\n",
    "            elif print_what == 'prompt':\n",
    "                print('Q:', level['prompt'])\n",
    "            print('ChatGPT A:', level['Response'])\n",
    "            print('Real A:   ', level['A'])\n",
    "\n",
    "            print('')\n",
    "            #if 'Error' in level:\n",
    "            #    import sys; sys.exit()\n",
    "    return qa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "c6eb280d-d109-4a03-8a76-ea47d5f8960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "with open(key_file,'r') as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=api_key.strip(),  # this is also the default, it can be omitted\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8957d21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/jsons/nclust_3_trial9.json',\n",
       " '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/jsons/nclust_5_trial3.json',\n",
       " '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/jsons/nclust_2_trial0.json']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsons_to_parse = glob(jsons_dir + '/*.json')\n",
    "jsons_to_parse[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb0608b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on 0 of 2\n",
      "have file already: /Users/jnaiman/LLM_VQA_JCDL2025/example_hists/LLM_outputs/chatgpt_api/nclust_3_trial9.pickle\n",
      "on 1 of 2\n",
      "   on question: how many bars are there in the specified figure panel? Please format the output as a json as {\"nbars\":\"\"} for this figure panel, where the \"nbars\" value should be an integer.\n",
      "   on question: what are the maximum data values in this figure panel?  Please format the output as a json as {\"maximum x\":\"\"} for this figure panel, where the \"maximum\" value should be a float, calculated from the data values used to create the plot.\n",
      "   on question: what are the mean data values in this figure panel?  Please format the output as a json as {\"mean x\":\"\"} for this figure panel, where the \"mean\" value should be a float, calculated from the data values used to create the plot.\n",
      "   on question: what are the median data values in this figure panel?  Please format the output as a json as {\"median x\":\"\"} for this figure panel, where the \"median\" value should be a float, calculated from the data values used to create the plot.\n",
      "   on question: what are the minimum data values in this figure panel?  Please format the output as a json as {\"minimum x\":\"\"} for this figure panel, where the \"minimum\" value should be a float, calculated from the data values used to create the plot.\n",
      "   on question: how many gaussians were used to generate the data for the plot in the figure panel? Please format the output as a json as {\"ngaussians\":\"\"} for this figure panel, where the \"ngaussians\" value should be an integer.\n",
      "Q: I am going to show you an image. Here is the image: [Image: <ENCODED IMAGE>]. Now, how many bars are there in the specified figure panel? Please format the output as a json as {\"nbars\":\"\"} for this figure panel, where the \"nbars\" value should be an integer.\n",
      "ChatGPT A: {'nbars': 20}\n",
      "Real A:    50\n",
      "\n",
      "Q: I am going to show you an image. Here is the image: [Image: <ENCODED IMAGE>]. Now, what are the maximum data values in this figure panel?  Please format the output as a json as {\"maximum x\":\"\"} for this figure panel, where the \"maximum\" value should be a float, calculated from the data values used to create the plot.\n",
      "ChatGPT A: {'maximum x': 0.5}\n",
      "Real A:    0.531999058363311\n",
      "\n",
      "Q: I am going to show you an image. Here is the image: [Image: <ENCODED IMAGE>]. Now, what are the minimum data values in this figure panel?  Please format the output as a json as {\"minimum x\":\"\"} for this figure panel, where the \"minimum\" value should be a float, calculated from the data values used to create the plot.\n",
      "ChatGPT A: {'minimum x': 0.0}\n",
      "Real A:    -0.44113410366866\n",
      "\n",
      "Q: I am going to show you an image. Here is the image: [Image: <ENCODED IMAGE>]. Now, how many gaussians were used to generate the data for the plot in the figure panel? Please format the output as a json as {\"ngaussians\":\"\"} for this figure panel, where the \"ngaussians\" value should be an integer.\n",
      "ChatGPT A: {'ngaussians': 5}\n",
      "Real A:    5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iMax = 2\n",
    "verbose = False\n",
    "test_run = False # run w/o actually pinging openai\n",
    "restart = False\n",
    "model =\"gpt-4o-mini\"\n",
    "\n",
    "fac = 0.5\n",
    "for ijson,json_path in enumerate(jsons_to_parse):\n",
    "    if ijson >= iMax:\n",
    "        continue\n",
    "\n",
    "    print('on', ijson, 'of', iMax)\n",
    "\n",
    "    # get image and base json\n",
    "    img_path = imgs_dir + json_path.split('/')[-1].removesuffix('.json') + '.' + img_format\n",
    "    encoded_image, base_json, err = get_img_json_pair(img_path, json_path, dir_api, \n",
    "                                                      fac=fac, restart=restart,\n",
    "                                                      tmp_dir=tmp_dir)\n",
    "\n",
    "    if err:\n",
    "        continue\n",
    "\n",
    "\n",
    "    ###### create QA ########\n",
    "    qa = []\n",
    "    \n",
    "    for k,v in base_json['VQA']['Level 1']['Figure-level questions'].items():\n",
    "        out = {'Q':v['Q'], 'A':v['A'], 'Level':'Level 1', 'type':'Figure-level questions', 'Response':\"\"}\n",
    "        qa.append(out)\n",
    "    \n",
    "    # what kinds?\n",
    "    types = ['(words + list)', '(words)']\n",
    "    \n",
    "    # get uniques\n",
    "    level_parse = 'Level 1'\n",
    "    plot_level = 'Plot-level questions'\n",
    "    qa = parse_qa(level_parse, plot_level, qa, base_json['VQA'], types)\n",
    "    \n",
    "    level_parse = 'Level 2'\n",
    "    plot_level = 'Plot-level questions'\n",
    "    qa = parse_qa(level_parse, plot_level, qa, base_json['VQA'], types)\n",
    "    \n",
    "    level_parse = 'Level 3'\n",
    "    plot_level = 'Plot-level questions'\n",
    "    qa = parse_qa(level_parse, plot_level, qa, base_json['VQA'], types)\n",
    "\n",
    "    responses = []\n",
    "    for question_list in qa:\n",
    "        response, prompt = send_to_chatgpt(question_list, client, img_path, encoded_image,\n",
    "                    model = model, \n",
    "                    test_run = test_run)\n",
    "        responses.append(response)\n",
    "        question_list['prompt'] = prompt\n",
    "\n",
    "    # parse for errors\n",
    "    qa = parse_for_errors(qa)\n",
    "\n",
    "    # dump to file\n",
    "    if not test_run:\n",
    "        with open(dir_api + json_path.split('/')[-1].removesuffix('.json')+ '.pickle', 'wb') as ff:\n",
    "            pickle.dump([qa, model], ff)\n",
    "    else:\n",
    "        print('Would store at:', dir_api + json_path.split('/')[-1].removesuffix('.json')+ '.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042a456-e8cf-484f-958c-75cb9b0f9606",
   "metadata": {},
   "source": [
    "## Look at data\n",
    "\n",
    "Check out one, if you wanna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "6974b8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/LLM_outputs/chatgpt_api/nclust_5_trial3.pickle',\n",
       " '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/LLM_outputs/chatgpt_api/nclust_3_trial9.pickle']"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickles = glob(dir_api + '*.pickle')\n",
    "pickles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "636df058",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifile = 0\n",
    "with open(pickles[ifile], 'rb') as f:\n",
    "    qa_in = pickle.load(f)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "11948f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/LLM_outputs/chatgpt_api/nclust_5_trial3.pickle\n",
      "*********\n",
      "Prompt: I am going to show you an image. Here is the image: [Image: <ENCODED IMAGE>]. Now, how many bars are there in the specified figure panel? Please format the output as a json as {\"nbars\":\"\"} for this figure panel, where the \"nbars\" value should be an integer.\n",
      "   Real A: 50\n",
      "ChatGPT A: {'nbars': 20}\n",
      "\n",
      "Prompt: I am going to show you an image. Here is the image: [Image: <ENCODED IMAGE>]. Now, what are the maximum data values in this figure panel?  Please format the output as a json as {\"maximum x\":\"\"} for this figure panel, where the \"maximum\" value should be a float, calculated from the data values used to create the plot.\n",
      "   Real A: 0.531999058363311\n",
      "ChatGPT A: {'maximum x': 0.5}\n",
      "\n",
      "Prompt: I am going to show you an image. Here is the image: [Image: <ENCODED IMAGE>]. Now, what are the mean data values in this figure panel?  Please format the output as a json as {\"mean x\":\"\"} for this figure panel, where the \"mean\" value should be a float, calculated from the data values used to create the plot.\n",
      "   Real A: -0.03567869693057275\n",
      "ChatGPT A: I'm unable to analyze the specific data values from the figure panel to compute the mean. However, if you have the data values available, I can help you calculate the mean. Please provide the relevant data points.\n",
      "\n",
      "Prompt: I am going to show you an image. Here is the image: [Image: <ENCODED IMAGE>]. Now, what are the median data values in this figure panel?  Please format the output as a json as {\"median x\":\"\"} for this figure panel, where the \"median\" value should be a float, calculated from the data values used to create the plot.\n",
      "   Real A: -0.11391004457919605\n",
      "ChatGPT A: I'm unable to extract specific data values directly from images, including figures and plots. However, if you provide me with the data points or additional context, I can assist you in calculating the median or any other statistics you need.\n",
      "\n",
      "Prompt: I am going to show you an image. Here is the image: [Image: <ENCODED IMAGE>]. Now, what are the minimum data values in this figure panel?  Please format the output as a json as {\"minimum x\":\"\"} for this figure panel, where the \"minimum\" value should be a float, calculated from the data values used to create the plot.\n",
      "   Real A: -0.44113410366866\n",
      "ChatGPT A: {'minimum x': 0.0}\n",
      "\n",
      "Prompt: I am going to show you an image. Here is the image: [Image: <ENCODED IMAGE>]. Now, how many gaussians were used to generate the data for the plot in the figure panel? Please format the output as a json as {\"ngaussians\":\"\"} for this figure panel, where the \"ngaussians\" value should be an integer.\n",
      "   Real A: 5\n",
      "ChatGPT A: {'ngaussians': 5}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pickles[ifile])\n",
    "print('*********')\n",
    "for qa_pairs in qa_in:\n",
    "    print('Prompt:', qa_pairs['prompt'])\n",
    "    print('   Real A:', qa_pairs['A'])\n",
    "    print('ChatGPT A:', qa_pairs['Response'])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d309dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JCDL2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7fa3ee9-62ba-4849-9f83-8584139e295f",
   "metadata": {},
   "source": [
    "## ChatGPT\n",
    "\n",
    "Ask VQA to ChatGPT."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6baf77f-b887-437d-b0c6-20cde719d906",
   "metadata": {},
   "source": [
    "Docs:\n",
    "\n",
    "* how to upload images: https://platform.openai.com/docs/guides/vision\n",
    "* types of models: https://stackoverflow.com/questions/75774873/openai-api-error-this-is-a-chat-model-and-not-supported-in-the-v1-completions\n",
    "* token limits: https://platform.openai.com/settings/organization/limits\n",
    "* models: https://platform.openai.com/docs/models/model-endpoint-compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f25dda74-0b62-4a27-9566-aaaa27727c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017cce17-f4e8-4df6-a3b5-2edef3714601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####dir_api = '/Users/jnaiman/Dropbox/Paper_JCDL2025/chatgpt_api/' # where to store API results\n",
    "\n",
    "dir_api = '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/LLM_outputs/chatgpt_api/' #store API results for example-hists\n",
    "model = \"gpt-5-nano-2025-08-07\"\n",
    "temperature = 1.0\n",
    "\n",
    "# NOT SUPPORTED\n",
    "# dir_api = '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/LLM_outputs/chatgpt_api_t0/' #store API results for example-hists\n",
    "# model = \"gpt-5-nano-2025-08-07\"\n",
    "# temperature = 0.0 # more deterministic\n",
    "\n",
    "# dir_api = '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/LLM_outputs/chatgpt_api_mini/' #store API results for example-hists\n",
    "# model = \"gpt-5-mini-2025-08-07\"\n",
    "# temperature = 1.0\n",
    "\n",
    "key_file = '/Users/jnaiman/.openai/key.txt'\n",
    "\n",
    "jsons_dir = '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/jsons/' # directory where jsons created with figure are stored\n",
    "imgs_dir = '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/imgs/' # where images are stored\n",
    "\n",
    "# for saving temp images for reading in\n",
    "tmp_dir = '/Users/jnaiman/Downloads/tmp/'\n",
    "\n",
    "img_format = 'jpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a2cf577c-4a80-4fbf-b4a0-8a0a61d16b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import base64\n",
    "from openai import OpenAI\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from sys import path\n",
    "path.append('../')\n",
    "from utils.llm_utils import parse_qa, load_image, get_img_json_pair, parse_for_errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d2d69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_chatgpt(question_list, client, image_path, encoded_image,\n",
    "                    model =\"gpt-4o-mini\", \n",
    "                    tmp_dir = '/Users/jnaiman/Downloads/tmp/',\n",
    "                    test_run = True, fac=1.0, img_format='png',\n",
    "                    verbose=True, temperature=1.0,\n",
    "                    subset_questions_by_keys = None):\n",
    "    # models as of ~May 2024\n",
    "    #model=\"gpt-4\",\n",
    "    #model=\"gpt-4o\",\n",
    "    #model =\"gpt-4o-mini\",\n",
    "    #model =\"gpt-3.5-turbo\",\n",
    "    #model=\"gpt-3.5-turbo-instruct\",\n",
    "    \"\"\"\n",
    "    subset_questions_by_keys : only use a subset of questions to ping to model, e.g. ['median', 'how many gaussians']\n",
    "    \"\"\"\n",
    "\n",
    "    iFac = 1.0 # just in case we want to progressively make the image smaller\n",
    "    success = False\n",
    "    is_subset = False\n",
    "    while not success:\n",
    "        try:\n",
    "            # current question format is: ['persona', 'context','question', 'format'] (see readme in example_hists)\n",
    "            question = question_list['context'] + \" \" + question_list['question'] + \" \" + question_list['format']\n",
    "            # lowercase the first word, just in case\n",
    "            question = question.lstrip() # no whitespace\n",
    "            question = question[0].lower() + question[1:]\n",
    "            if verbose: print('   on question:',question)\n",
    "            # # Prepare the API request\n",
    "            # prompt = f\"I am going to show you an image. Here is the image: [Image: {encoded_image}]. Now, {question}\"\n",
    "            # prompt_save = f\"I am going to show you an image. Here is the image: [Image: <ENCODED IMAGE>]. Now, {question}\"\n",
    "            prompt = f\"I am going to show you an image. Now, {question}\"\n",
    "            prompt_save = f\"I am going to show you an image. Now, {question}\"\n",
    "\n",
    "            if subset_questions_by_keys is not None and type(subset_questions_by_keys) == type([]): # make sure list\n",
    "                for s in subset_questions_by_keys:\n",
    "                    if s in question:\n",
    "                        is_subset = True\n",
    "            else:\n",
    "                is_subset = True\n",
    "            \n",
    "            if not test_run and is_subset:\n",
    "                # Send the request to the GPT-4o API\n",
    "                response = client.chat.completions.create(\n",
    "                    model = model,\n",
    "                    messages=[\n",
    "                        {\"role\": \"system\", \"content\": question_list['persona']},\n",
    "                        {\"role\":\"user\", \"content\": [\n",
    "                            {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": prompt\n",
    "                            },\n",
    "                            {\n",
    "                            \"type\": \"image_url\",\n",
    "                            \"image_url\": {\n",
    "                                #\"url\": f\"data:image/jpeg;base64,{encoded_image}\" # PNG encoding??? does thsi need to change??\n",
    "                                \"url\": f\"data:image/{img_format};base64,{encoded_image}\" \n",
    "                            }\n",
    "                            }\n",
    "                        ]\n",
    "                        }\n",
    "                    ],\n",
    "                    temperature=temperature\n",
    "                )\n",
    "                success = True\n",
    "            elif not is_subset:\n",
    "                response = 'Not asked'\n",
    "                success = True\n",
    "            else:\n",
    "                success = True\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            new_fac = fac/iFac\n",
    "            print('new fac = ', new_fac)\n",
    "            encoded_image = load_image(image_path,fac=new_fac, tmp_dir=tmp_dir)\n",
    "            iFac += 1\n",
    "    \n",
    "    if not test_run and is_subset:\n",
    "        # Get the response from the API\n",
    "        answer = response.choices[0].message.content\n",
    "        question_list['raw answer'] = answer\n",
    "        # format answer\n",
    "        answer_format = answer.replace(\"```json\\n\",'').replace(\"\\n```\",'')\n",
    "        try:\n",
    "            question_list['Response'] = json.loads(answer_format)\n",
    "        except:\n",
    "            question_list['Response'] = answer_format\n",
    "            question_list['Error'] = 'JSON formatting'\n",
    "        question_list['Response String'] = answer_format\n",
    "        success = True\n",
    "    elif not is_subset:\n",
    "        answer = response\n",
    "        question_list['raw answer'] = answer\n",
    "        question_list['Response'] = answer\n",
    "        question_list['Response String'] = answer\n",
    "        success = True\n",
    "    else:\n",
    "        question_list['Response'] = 'TEST RUN'\n",
    "        question_list['Response String'] = 'TEST RUN'\n",
    "\n",
    "    return question_list, prompt_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "04dab3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_qa(pickle_file, qa_in, subset_questions_by_keys=None, showNotAsked=False):\n",
    "    if subset_questions_by_keys is not None:\n",
    "        print('---------- ASKED ----------')\n",
    "    print(pickle_file)\n",
    "    print('*********')\n",
    "    for qa_pairs in qa_in:\n",
    "        hasSub = False\n",
    "        if subset_questions_by_keys is not None and type(subset_questions_by_keys) == type([]):\n",
    "            for s in subset_questions_by_keys:\n",
    "                if s in qa_pairs['prompt']:\n",
    "                    hasSub = True\n",
    "        else:\n",
    "            hasSub = True\n",
    "\n",
    "        if hasSub:\n",
    "            print('Prompt:', qa_pairs['prompt'])\n",
    "            print('   Real A:', qa_pairs['A'])\n",
    "            print('ChatGPT A:', qa_pairs['Response'])\n",
    "            print('')\n",
    "\n",
    "    if subset_questions_by_keys is not None and showNotAsked:\n",
    "        print('')\n",
    "        print('')\n",
    "        print('------------ NOT ASKED -----------')\n",
    "        for qa_pairs in qa_in:\n",
    "            hasSub = False\n",
    "            if subset_questions_by_keys is not None and type(subset_questions_by_keys) == type([]):\n",
    "                for s in subset_questions_by_keys:\n",
    "                    if s in qa_pairs['prompt']:\n",
    "                        hasSub = True\n",
    "\n",
    "            if not hasSub:\n",
    "                print('Prompt:', qa_pairs['prompt'])\n",
    "                print('   Real A:', qa_pairs['A'])\n",
    "                print('ChatGPT A:', qa_pairs['Response'])\n",
    "                print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c6eb280d-d109-4a03-8a76-ea47d5f8960c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "with open(key_file,'r') as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "client = OpenAI(\n",
    "  api_key=api_key.strip(),  # this is also the default, it can be omitted\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "8957d21b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/jsons/id_0077.json',\n",
       " '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/jsons/id_0020.json',\n",
       " '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/jsons/id_0036.json']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsons_to_parse = glob(jsons_dir + '/*.json')\n",
    "jsons_to_parse[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f8382b9",
   "metadata": {},
   "source": [
    "Look at a possible questions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "5e019153",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Level 1': {'Figure-level questions': {},\n",
       "  'Plot-level questions': {'nbars': {'plot0': {'Q': 'You are a helpful assistant that can analyze images.  How many bars are there in the specified figure panel? Please format the output as a json as {\"nbars\":\"\"} for this figure panel, where the \"nbars\" value should be an integer.',\n",
       "     'A': {'nbars': 20},\n",
       "     'persona': 'You are a helpful assistant that can analyze images.',\n",
       "     'context': '',\n",
       "     'question': 'How many bars are there in the specified figure panel?',\n",
       "     'format': 'Please format the output as a json as {\"nbars\":\"\"} for this figure panel, where the \"nbars\" value should be an integer.'}}}},\n",
       " 'Level 2': {'Plot-level questions': {'minimum': {'plot0': {'Q': 'You are a helpful assistant that can analyze images.  What is the minimum value of the data in this figure panel?  Please format the output as a json as {\"minimum x\":\"\"} for this figure panel, where the \"minimum x\" value should be a float, calculated from the data values used to create the plot.',\n",
       "     'A': {'minimum': {'minimum x': -0.9039030182959754}},\n",
       "     'persona': 'You are a helpful assistant that can analyze images.',\n",
       "     'context': '',\n",
       "     'question': 'What is the minimum value of the data in this figure panel? ',\n",
       "     'format': 'Please format the output as a json as {\"minimum x\":\"\"} for this figure panel, where the \"minimum x\" value should be a float, calculated from the data values used to create the plot.'}},\n",
       "   'maximum': {'plot0': {'Q': 'You are a helpful assistant that can analyze images.  What is the maximum value of the data in this figure panel?  Please format the output as a json as {\"maximum x\":\"\"} for this figure panel, where the \"maximum x\" value should be a float, calculated from the data values used to create the plot.',\n",
       "     'A': {'maximum': {'maximum x': -0.1500760324875851}},\n",
       "     'persona': 'You are a helpful assistant that can analyze images.',\n",
       "     'context': '',\n",
       "     'question': 'What is the maximum value of the data in this figure panel? ',\n",
       "     'format': 'Please format the output as a json as {\"maximum x\":\"\"} for this figure panel, where the \"maximum x\" value should be a float, calculated from the data values used to create the plot.'}},\n",
       "   'median': {'plot0': {'Q': 'You are a helpful assistant that can analyze images.  What is the median value of the data in this figure panel?  Please format the output as a json as {\"median x\":\"\"} for this figure panel, where the \"median x\" value should be a float, calculated from the data values used to create the plot.',\n",
       "     'A': {'median': {'median x': -0.4723913577551772}},\n",
       "     'persona': 'You are a helpful assistant that can analyze images.',\n",
       "     'context': '',\n",
       "     'question': 'What is the median value of the data in this figure panel? ',\n",
       "     'format': 'Please format the output as a json as {\"median x\":\"\"} for this figure panel, where the \"median x\" value should be a float, calculated from the data values used to create the plot.'}},\n",
       "   'mean': {'plot0': {'Q': 'You are a helpful assistant that can analyze images.  What is the mean value of the data in this figure panel?  Please format the output as a json as {\"mean x\":\"\"} for this figure panel, where the \"mean x\" value should be a float, calculated from the data values used to create the plot.',\n",
       "     'A': {'mean': {'mean x': -0.49127551258688784}},\n",
       "     'persona': 'You are a helpful assistant that can analyze images.',\n",
       "     'context': '',\n",
       "     'question': 'What is the mean value of the data in this figure panel? ',\n",
       "     'format': 'Please format the output as a json as {\"mean x\":\"\"} for this figure panel, where the \"mean x\" value should be a float, calculated from the data values used to create the plot.'}}}},\n",
       " 'Level 3': {'Plot-level questions': {'ngaussians': {'plot0': {'Q': 'You are a helpful assistant that can analyze images.  How many gaussians were used to generate the data for the plot in the figure panel? Please format the output as a json as {\"ngaussians\":\"\"} for this figure panel, where the \"ngaussians\" value should be an integer. Please choose an integer number from 1 to 5.',\n",
       "     'A': {'ngaussians': 2},\n",
       "     'persona': 'You are a helpful assistant that can analyze images.',\n",
       "     'context': '',\n",
       "     'question': 'How many gaussians were used to generate the data for the plot in the figure panel?',\n",
       "     'format': 'Please format the output as a json as {\"ngaussians\":\"\"} for this figure panel, where the \"ngaussians\" value should be an integer. Please choose an integer number from 1 to 5.'}}}}}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_path = jsons_to_parse[0]\n",
    "verbose = False\n",
    "restart = False\n",
    "\n",
    "fac = 0.5\n",
    "\n",
    "img_path = imgs_dir + json_path.split('/')[-1].removesuffix('.json') + '.' + img_format\n",
    "encoded_image, img_format_media, base_json, err = get_img_json_pair(img_path, json_path, dir_api, \n",
    "                                                    fac=fac, restart=restart,\n",
    "                                                    tmp_dir=tmp_dir)\n",
    "\n",
    "base_json['VQA']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "855459bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb0608b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on 1 of 80\n",
      "   on question: how many bars are there in the specified figure panel? Please format the output as a json as {\"nbars\":\"\"} for this figure panel, where the \"nbars\" value should be an integer.\n",
      "   on question: what is the maximum value of the data in this figure panel?  Please format the output as a json as {\"maximum x\":\"\"} for this figure panel, where the \"maximum x\" value should be a float, calculated from the data values used to create the plot.\n",
      "   on question: what is the mean value of the data in this figure panel?  Please format the output as a json as {\"mean x\":\"\"} for this figure panel, where the \"mean x\" value should be a float, calculated from the data values used to create the plot.\n",
      "   on question: what is the median value of the data in this figure panel?  Please format the output as a json as {\"median x\":\"\"} for this figure panel, where the \"median x\" value should be a float, calculated from the data values used to create the plot.\n",
      "Error code: 400 - {'error': {'message': \"Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}\n",
      "new fac =  1.0\n",
      "   on question: what is the median value of the data in this figure panel?  Please format the output as a json as {\"median x\":\"\"} for this figure panel, where the \"median x\" value should be a float, calculated from the data values used to create the plot.\n",
      "Error code: 400 - {'error': {'message': \"Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}\n",
      "new fac =  0.5\n",
      "   on question: what is the median value of the data in this figure panel?  Please format the output as a json as {\"median x\":\"\"} for this figure panel, where the \"median x\" value should be a float, calculated from the data values used to create the plot.\n",
      "Error code: 400 - {'error': {'message': \"Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}\n",
      "new fac =  0.3333333333333333\n",
      "   on question: what is the median value of the data in this figure panel?  Please format the output as a json as {\"median x\":\"\"} for this figure panel, where the \"median x\" value should be a float, calculated from the data values used to create the plot.\n",
      "Error code: 400 - {'error': {'message': \"Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}\n",
      "new fac =  0.25\n",
      "   on question: what is the median value of the data in this figure panel?  Please format the output as a json as {\"median x\":\"\"} for this figure panel, where the \"median x\" value should be a float, calculated from the data values used to create the plot.\n",
      "Error code: 400 - {'error': {'message': \"Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}\n",
      "new fac =  0.2\n",
      "   on question: what is the median value of the data in this figure panel?  Please format the output as a json as {\"median x\":\"\"} for this figure panel, where the \"median x\" value should be a float, calculated from the data values used to create the plot.\n",
      "Error code: 400 - {'error': {'message': \"Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}\n",
      "new fac =  0.16666666666666666\n",
      "   on question: what is the median value of the data in this figure panel?  Please format the output as a json as {\"median x\":\"\"} for this figure panel, where the \"median x\" value should be a float, calculated from the data values used to create the plot.\n",
      "Error code: 400 - {'error': {'message': \"Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}\n",
      "new fac =  0.14285714285714285\n",
      "   on question: what is the median value of the data in this figure panel?  Please format the output as a json as {\"median x\":\"\"} for this figure panel, where the \"median x\" value should be a float, calculated from the data values used to create the plot.\n",
      "Error code: 400 - {'error': {'message': \"Unsupported value: 'temperature' does not support 0.0 with this model. Only the default (1) value is supported.\", 'type': 'invalid_request_error', 'param': 'temperature', 'code': 'unsupported_value'}}\n",
      "new fac =  0.125\n",
      "   on question: what is the median value of the data in this figure panel?  Please format the output as a json as {\"median x\":\"\"} for this figure panel, where the \"median x\" value should be a float, calculated from the data values used to create the plot.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 51\u001b[39m\n\u001b[32m     49\u001b[39m responses = []\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m question_list \u001b[38;5;129;01min\u001b[39;00m qa:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     response, prompt = \u001b[43msend_to_chatgpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquestion_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoded_image\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m                \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_format\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_format_media\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m                \u001b[49m\u001b[43mtest_run\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_run\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubset_questions_by_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubset_questions_by_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m     responses.append(response)\n\u001b[32m     55\u001b[39m     question_list[\u001b[33m'\u001b[39m\u001b[33mprompt\u001b[39m\u001b[33m'\u001b[39m] = prompt\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[60]\u001b[39m\u001b[32m, line 41\u001b[39m, in \u001b[36msend_to_chatgpt\u001b[39m\u001b[34m(question_list, client, image_path, encoded_image, model, tmp_dir, test_run, fac, img_format, verbose, temperature, subset_questions_by_keys)\u001b[39m\n\u001b[32m     37\u001b[39m     is_subset = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m test_run \u001b[38;5;129;01mand\u001b[39;00m is_subset:\n\u001b[32m     40\u001b[39m     \u001b[38;5;66;03m# Send the request to the GPT-4o API\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     response = \u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mchat\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompletions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     43\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msystem\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion_list\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mpersona\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     45\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrole\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcontent\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m                \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     47\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtext\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[43m                \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtype\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage_url\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     52\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mimage_url\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m#\"url\": f\"data:image/jpeg;base64,{encoded_image}\" # PNG encoding??? does thsi need to change??\u001b[39;49;00m\n\u001b[32m     54\u001b[39m \u001b[43m                    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdata:image/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mimg_format\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m;base64,\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mencoded_image\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m                \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m            \u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m     success = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_subset:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/JCDL2025/lib/python3.12/site-packages/openai/_utils/_utils.py:286\u001b[39m, in \u001b[36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    284\u001b[39m             msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[32m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    285\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/JCDL2025/lib/python3.12/site-packages/openai/resources/chat/completions/completions.py:1147\u001b[39m, in \u001b[36mCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   1101\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   1102\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   1103\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1144\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = NOT_GIVEN,\n\u001b[32m   1145\u001b[39m ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[32m   1146\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m1147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1148\u001b[39m \u001b[43m        \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/chat/completions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1149\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1150\u001b[39m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m   1151\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmessages\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1152\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1153\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43maudio\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1154\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfrequency_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1155\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunction_call\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1156\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mfunctions\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1157\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogit_bias\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1158\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mlogprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1159\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_completion_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1160\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmax_tokens\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1161\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmetadata\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1162\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodalities\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1163\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1164\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mparallel_tool_calls\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1165\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprediction\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1166\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mpresence_penalty\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1167\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mprompt_cache_key\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1168\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mreasoning_effort\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mreasoning_effort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1169\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mresponse_format\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1170\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msafety_identifier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43msafety_identifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1171\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mseed\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1172\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mservice_tier\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1173\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstop\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1174\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstore\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1175\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1176\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstream_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1177\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtemperature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1178\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtool_choice\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1179\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtools\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1180\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_logprobs\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1181\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtop_p\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43muser\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mverbosity\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbosity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1184\u001b[39m \u001b[43m                \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweb_search_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mweb_search_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1185\u001b[39m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1186\u001b[39m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsStreaming\u001b[49m\n\u001b[32m   1187\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\n\u001b[32m   1188\u001b[39m \u001b[43m            \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[43m.\u001b[49m\u001b[43mCompletionCreateParamsNonStreaming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1189\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1190\u001b[39m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1191\u001b[39m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m=\u001b[49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1193\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m=\u001b[49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1194\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1195\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/JCDL2025/lib/python3.12/site-packages/openai/_base_client.py:1259\u001b[39m, in \u001b[36mSyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[39m\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1246\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1247\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1254\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1255\u001b[39m ) -> ResponseT | _StreamT:\n\u001b[32m   1256\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1257\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=to_httpx_files(files), **options\n\u001b[32m   1258\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1259\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/JCDL2025/lib/python3.12/site-packages/openai/_base_client.py:982\u001b[39m, in \u001b[36mSyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m    980\u001b[39m response = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    981\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    983\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    984\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_should_stream_response_body\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    985\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    986\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    987\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m httpx.TimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    988\u001b[39m     log.debug(\u001b[33m\"\u001b[39m\u001b[33mEncountered httpx.TimeoutException\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/JCDL2025/lib/python3.12/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/JCDL2025/lib/python3.12/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/JCDL2025/lib/python3.12/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/JCDL2025/lib/python3.12/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/JCDL2025/lib/python3.12/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/JCDL2025/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/JCDL2025/lib/python3.12/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/JCDL2025/lib/python3.12/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/JCDL2025/lib/python3.12/site-packages/httpcore/_sync/http11.py:136\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    134\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m Trace(\u001b[33m\"\u001b[39m\u001b[33mresponse_closed\u001b[39m\u001b[33m\"\u001b[39m, logger, request) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    135\u001b[39m         \u001b[38;5;28mself\u001b[39m._response_closed()\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/JCDL2025/lib/python3.12/site-packages/httpcore/_sync/http11.py:106\u001b[39m, in \u001b[36mHTTP11Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m     95\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m     98\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m     99\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m    100\u001b[39m     (\n\u001b[32m    101\u001b[39m         http_version,\n\u001b[32m    102\u001b[39m         status,\n\u001b[32m    103\u001b[39m         reason_phrase,\n\u001b[32m    104\u001b[39m         headers,\n\u001b[32m    105\u001b[39m         trailing_data,\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     ) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response_headers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m     trace.return_value = (\n\u001b[32m    108\u001b[39m         http_version,\n\u001b[32m    109\u001b[39m         status,\n\u001b[32m    110\u001b[39m         reason_phrase,\n\u001b[32m    111\u001b[39m         headers,\n\u001b[32m    112\u001b[39m     )\n\u001b[32m    114\u001b[39m network_stream = \u001b[38;5;28mself\u001b[39m._network_stream\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/JCDL2025/lib/python3.12/site-packages/httpcore/_sync/http11.py:177\u001b[39m, in \u001b[36mHTTP11Connection._receive_response_headers\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    174\u001b[39m timeout = timeouts.get(\u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h11.Response):\n\u001b[32m    179\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/JCDL2025/lib/python3.12/site-packages/httpcore/_sync/http11.py:217\u001b[39m, in \u001b[36mHTTP11Connection._receive_event\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    214\u001b[39m     event = \u001b[38;5;28mself\u001b[39m._h11_state.next_event()\n\u001b[32m    216\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;129;01mis\u001b[39;00m h11.NEED_DATA:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    218\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\n\u001b[32m    219\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    221\u001b[39m     \u001b[38;5;66;03m# If we feed this case through h11 we'll raise an exception like:\u001b[39;00m\n\u001b[32m    222\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    223\u001b[39m     \u001b[38;5;66;03m#     httpcore.RemoteProtocolError: can't handle event type\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m     \u001b[38;5;66;03m# perspective. Instead we handle this case distinctly and treat\u001b[39;00m\n\u001b[32m    228\u001b[39m     \u001b[38;5;66;03m# it as a ConnectError.\u001b[39;00m\n\u001b[32m    229\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._h11_state.their_state == h11.SEND_RESPONSE:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/JCDL2025/lib/python3.12/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/JCDL2025/lib/python3.12/ssl.py:1232\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1228\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1229\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1230\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1231\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1232\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1234\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/JCDL2025/lib/python3.12/ssl.py:1105\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1103\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1104\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1107\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "iMax = 150 # should be 80\n",
    "verbose = False\n",
    "test_run = False # run w/o actually pinging openai\n",
    "restart = False\n",
    "#model =\"gpt-4o-mini\"\n",
    "\n",
    "subset_questions_by_keys = ['median', 'ngaussians'] # set to None to do all questions\n",
    "\n",
    "fac = 0.5\n",
    "for ijson,json_path in enumerate(jsons_to_parse):\n",
    "    if ijson >= iMax:\n",
    "        continue\n",
    "\n",
    "    print('on', ijson+1, 'of', min(iMax,len(jsons_to_parse)))\n",
    "\n",
    "    # get image and base json\n",
    "    img_path = imgs_dir + json_path.split('/')[-1].removesuffix('.json') + '.' + img_format\n",
    "    encoded_image, img_format_media, base_json, err = get_img_json_pair(img_path, json_path, dir_api, \n",
    "                                                      fac=fac, restart=restart,\n",
    "                                                      tmp_dir=tmp_dir)\n",
    "\n",
    "    if err:\n",
    "        continue\n",
    "\n",
    "\n",
    "    ###### create QA ########\n",
    "    qa = []\n",
    "    \n",
    "    for k,v in base_json['VQA']['Level 1']['Figure-level questions'].items():\n",
    "        out = {'Q':v['Q'], 'A':v['A'], 'Level':'Level 1', 'type':'Figure-level questions', 'Response':\"\"}\n",
    "        qa.append(out)\n",
    "    \n",
    "    # what kinds?\n",
    "    types = ['(words + list)', '(words)']\n",
    "    \n",
    "    # get uniques\n",
    "    level_parse = 'Level 1'\n",
    "    plot_level = 'Plot-level questions'\n",
    "    qa = parse_qa(level_parse, plot_level, qa, base_json['VQA'], types)\n",
    "    \n",
    "    level_parse = 'Level 2'\n",
    "    plot_level = 'Plot-level questions'\n",
    "    qa = parse_qa(level_parse, plot_level, qa, base_json['VQA'], types)\n",
    "    \n",
    "    level_parse = 'Level 3'\n",
    "    plot_level = 'Plot-level questions'\n",
    "    qa = parse_qa(level_parse, plot_level, qa, base_json['VQA'], types)\n",
    "\n",
    "    responses = []\n",
    "    for question_list in qa:\n",
    "        response, prompt = send_to_chatgpt(question_list, client, img_path, encoded_image,\n",
    "                    model = model, img_format = img_format_media, temperature=temperature,\n",
    "                    test_run = test_run, subset_questions_by_keys=subset_questions_by_keys, \n",
    "                    fac=fac)\n",
    "        responses.append(response)\n",
    "        question_list['prompt'] = prompt\n",
    "\n",
    "    # parse for errors\n",
    "    qa = parse_for_errors(qa)\n",
    "\n",
    "    # dump to file\n",
    "    if not test_run:\n",
    "        with open(dir_api + json_path.split('/')[-1].removesuffix('.json')+ '.pickle', 'wb') as ff:\n",
    "            pickle.dump([qa, model], ff)\n",
    "        print(\"just saved:\", dir_api + json_path.split('/')[-1].removesuffix('.json')+ '.pickle')\n",
    "    else:\n",
    "        print('Would store at:', dir_api + json_path.split('/')[-1].removesuffix('.json')+ '.pickle')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9042a456-e8cf-484f-958c-75cb9b0f9606",
   "metadata": {},
   "source": [
    "## Look at data\n",
    "\n",
    "Check out one, if you wanna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6974b8e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/LLM_outputs/chatgpt_api/id_0039.pickle',\n",
       " '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/LLM_outputs/chatgpt_api/id_0044.pickle',\n",
       " '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/LLM_outputs/chatgpt_api/id_0027.pickle',\n",
       " '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/LLM_outputs/chatgpt_api/id_0056.pickle',\n",
       " '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/LLM_outputs/chatgpt_api/id_0035.pickle']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickles = glob(dir_api + '*.pickle')\n",
    "pickles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "636df058",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifile = 0\n",
    "with open(pickles[ifile], 'rb') as f:\n",
    "    qa_in = pickle.load(f)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "11948f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- ASKED ----------\n",
      "/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/LLM_outputs/chatgpt_api/id_0039.pickle\n",
      "*********\n",
      "Prompt: I am going to show you an image. Here is the image: [Image: <ENCODED IMAGE>]. Now, what are the median data values  in this figure panel?  Please format the output as a json as {\"median x\":\"\"} for this figure panel, where the \"median x\" value should be a float, calculated from the data values used to create the plot.\n",
      "   Real A: 0.2655385491960254\n",
      "ChatGPT A: {'median x': 0.27}\n",
      "\n",
      "Prompt: I am going to show you an image. Here is the image: [Image: <ENCODED IMAGE>]. Now, how many gaussians were used to generate the data for the plot in the figure panel? Please format the output as a json as {\"ngaussians\":\"\"} for this figure panel, where the \"ngaussians\" value should be an integer.\n",
      "   Real A: 3\n",
      "ChatGPT A: {'ngaussians': 3}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print_qa(pickles[ifile], qa_in, subset_questions_by_keys=subset_questions_by_keys, showNotAsked=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d309dd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JCDL2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7fa3ee9-62ba-4849-9f83-8584139e295f",
   "metadata": {},
   "source": [
    "## Claude.ai\n",
    "\n",
    "This is just some messing around with the `example_hists` files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6baf77f-b887-437d-b0c6-20cde719d906",
   "metadata": {},
   "source": [
    "### Docs:\n",
    "\n",
    "* in theory, there is documentation here: https://docs.anthropic.com/en/home, \n",
    "* but in reality I just asked \"Hey Claude.  How do I use your API through Python to upload an image and ask you questions about it?\" followed by \"Is there anyway to set the context?  For example, in chatgpt you have the \"\"role\": \"system\", \"content\"\" part of your message where you would say things like \"You are a helpful assistant in charge of automating a process\".  Or does one just incorporate that into the \"question\" part of the inputs?\" and starting building from there\n",
    "\n",
    "#### Key Points (cp-claude)\n",
    "\n",
    "* API Key: Get your API key from the [Anthropic Console](https://console.anthropic.com/) and set it as an environment variable or pass it directly\n",
    "* Supported formats: JPEG, PNG, GIF, and WebP images\n",
    "* Model: Use `claude-sonnet-4-20250514` for Claude Sonnet 4\n",
    "* Size limits: Images should be under 5MB and no larger than 8000x8000 pixels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68139a2",
   "metadata": {},
   "source": [
    "First, install anthropic api (also, see .yml file for the environment for this project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bb6c6253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d71599",
   "metadata": {},
   "source": [
    "Where are things stored/going to be stored?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acfdd16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example hists\n",
    "# dir_api = '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/LLM_outputs/claude_api/' #store API results for example-hists\n",
    "# jsons_dir = '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/jsons/' # directory where jsons created with figure are stored\n",
    "# imgs_dir = '/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/imgs/' # where images are stored\n",
    "\n",
    "# example lines\n",
    "dir_api = '/Users/jnaiman/LLM_VQA_JCDL2025/example_lines/LLM_outputs/claude_api/' #store API results for example-hists\n",
    "jsons_dir = '/Users/jnaiman/LLM_VQA_JCDL2025/example_lines/jsons/' # directory where jsons created with figure are stored\n",
    "imgs_dir = '/Users/jnaiman/LLM_VQA_JCDL2025/example_lines/imgs/' # where images are stored\n",
    "\n",
    "\n",
    "key_file = '/Users/jnaiman/.claudeai/key.txt'\n",
    "# for saving temp images for reading in\n",
    "tmp_dir = '/Users/jnaiman/Downloads/tmp/'\n",
    "\n",
    "img_format = 'jpeg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c959f5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import anthropic\n",
    "import base64\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "# debug\n",
    "from importlib import reload\n",
    "from anthropic import RateLimitError\n",
    "\n",
    "\n",
    "from sys import path\n",
    "path.append('../')\n",
    "import utils.llm_utils\n",
    "reload(utils.llm_utils)\n",
    "from utils.llm_utils import parse_qa, load_image, get_img_json_pair, parse_for_errors\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c219777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup\n",
    "with open(key_file,'r') as f:\n",
    "    api_key = f.read()\n",
    "\n",
    "client = anthropic.Anthropic(\n",
    "  api_key=api_key.strip(),  # this is also the default, it can be omitted\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ceaea9b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jnaiman/LLM_VQA_JCDL2025/example_lines/jsons/nclust_3_trial9.json',\n",
       " '/Users/jnaiman/LLM_VQA_JCDL2025/example_lines/jsons/nclust_5_trial3.json',\n",
       " '/Users/jnaiman/LLM_VQA_JCDL2025/example_lines/jsons/nclust_2_trial0.json']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jsons_to_parse = glob(jsons_dir + '/*.json')\n",
    "jsons_to_parse[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99b23a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_to_claude(question_list, client, image_path, encoded_image,\n",
    "                    model=\"claude-sonnet-4-20250514\",\n",
    "                    max_tokens=1000, temperature=0.1,\n",
    "                    media_type = 'image/png',\n",
    "                    tmp_dir = '/Users/jnaiman/Downloads/tmp/',\n",
    "                    test_run = True, fac=1.0, \n",
    "                    verbose=True,\n",
    "                    system_prompt = None, \n",
    "                    max_retries = 10, sleep_time=1):\n",
    "    \"\"\"\n",
    "    Sends the question to claude and collects response.clear\n",
    "\n",
    "    system_prompt : if None, then defaults to the overall system prompt generated with the questions.\n",
    "    \"\"\"\n",
    "    if system_prompt is None:\n",
    "        system_prompt = question_list['persona']\n",
    "\n",
    "    #print('system prompt:', system_prompt)\n",
    "\n",
    "    iFac = 1.0\n",
    "    success = False\n",
    "    #['persona', 'context','question', 'format']\n",
    "    attempt = 0\n",
    "    while not success and attempt < max_retries:\n",
    "        try:\n",
    "            question = question_list['context'] + \" \" + question_list['question'] + \" \" + question_list['format']\n",
    "            # lowercase the first word, just in case\n",
    "            question = question.lstrip() # no whitespace\n",
    "            question = question[0].lower() + question[1:]\n",
    "            if verbose: print('   on question:',question)\n",
    "            # Prepare the API request\n",
    "            prompt = f\"I am going to show you an image. Now, {question}\"\n",
    "            prompt_save = f\"I am going to show you an image. Now, {question}\"\n",
    "            ##question_list['prompt'] = prompt\n",
    "            \n",
    "            if not test_run:\n",
    "                # Send the request to the Claude api\n",
    "                response = client.messages.create(\n",
    "                    model = model,\n",
    "                    max_tokens=max_tokens,\n",
    "                    system = system_prompt,\n",
    "                    temperature=temperature,\n",
    "                    messages=[\n",
    "                        {\n",
    "                            \"role\": \"user\",\n",
    "                            \"content\": [\n",
    "                                {\n",
    "                                    \"type\": \"image\",\n",
    "                                    \"source\": {\n",
    "                                        \"type\": \"base64\",\n",
    "                                        \"media_type\": media_type,\n",
    "                                        \"data\": encoded_image,\n",
    "                                    },\n",
    "                                },\n",
    "                                {\n",
    "                                    \"type\": \"text\",\n",
    "                                    \"text\": prompt\n",
    "                                }\n",
    "                            ],\n",
    "                        }\n",
    "                    ]\n",
    "                )\n",
    "                success = True\n",
    "            else:\n",
    "                success = True\n",
    "        except RateLimitError as e:\n",
    "            if attempt < max_retries - 1:\n",
    "                # Exponential backoff with jitter\n",
    "                wait_time = sleep_time*(2 ** (attempt)) + np.random.uniform(0, 1)\n",
    "                print(f\"      Rate limit hit. Waiting {wait_time:.2f} seconds before retry {attempt + 1}\")\n",
    "                time.sleep(wait_time)\n",
    "                attempt += 1\n",
    "            else:\n",
    "                print(f\"Max retries exceeded. Error: {e}\")\n",
    "                raise\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            new_fac = fac/iFac\n",
    "            print('      new fac = ', new_fac)\n",
    "            encoded_image = load_image(image_path,fac=new_fac, tmp_dir=tmp_dir)\n",
    "            iFac += 1\n",
    "    \n",
    "    #print(response)\n",
    "    if not test_run:\n",
    "        # Get the response from the API\n",
    "        answer = response.content[0].text #response.choices[0].message.content\n",
    "        question_list['raw answer'] = answer\n",
    "        # also calculate usage\n",
    "        usage = response.usage\n",
    "        question_list['usage'] = usage\n",
    "        if verbose:\n",
    "            print(f\"      - Input tokens: {usage.input_tokens}\")\n",
    "            print(f\"      - Output tokens: {usage.output_tokens}\")\n",
    "            print(f\"      - Total tokens: {usage.input_tokens + usage.output_tokens}\")\n",
    "        # format answer\n",
    "        answer_format = answer.split('```json\"')[-1].split('\\n')[0].replace('\\n', '')\n",
    "        #answer.replace(\"```json\\n\",'').replace(\"\\n```\",'')\n",
    "        try:\n",
    "            question_list['Response'] = json.loads(answer_format)\n",
    "        except:\n",
    "            question_list['Response'] = answer_format\n",
    "            question_list['Error'] = 'JSON formatting'\n",
    "        question_list['Response String'] = answer_format\n",
    "        success = True\n",
    "    else:\n",
    "        question_list['Response'] = 'TEST RUN'\n",
    "        question_list['Response String'] = 'TEST RUN'\n",
    "\n",
    "    return question_list, prompt_save, system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f6a5b6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def send_to_claude_multi(questions, client, image_path, encoded_image,\n",
    "#                     model=\"claude-sonnet-4-20250514\",\n",
    "#                     max_tokens=4000, temperature=0.1, max_questions=10,\n",
    "#                     media_type = 'image/png',\n",
    "#                     tmp_dir = '/Users/jnaiman/Downloads/tmp/',\n",
    "#                     test_run = True, fac=1.0, \n",
    "#                     verbose=True,\n",
    "#                     system_prompt = None, \n",
    "#                     max_retries = 10, sleep_time=1):\n",
    "#     \"\"\"\n",
    "#     Sends the questions to claude and collects response.clear\n",
    "\n",
    "#     questions : list of questions to pull from, these are *all* qa pairs\n",
    "#     max_questions : the number of questions to use per figure at once\n",
    "\n",
    "#     system_prompt : if None, then defaults to the overall system prompt generated with the questions.\n",
    "#     \"\"\"\n",
    "\n",
    "#     # we will use the same system prompt for all quesionts\n",
    "#     if system_prompt is None:\n",
    "#         system_prompt = questions[0]['persona']\n",
    "\n",
    "#     for question_list in questions:\n",
    "\n",
    "\n",
    "#     #print('system prompt:', system_prompt)\n",
    "\n",
    "#     iFac = 1.0\n",
    "#     success = False\n",
    "#     #['persona', 'context','question', 'format']\n",
    "#     attempt = 0\n",
    "#     while not success and attempt < max_retries:\n",
    "#         try:\n",
    "#             question = question_list['context'] + \" \" + question_list['question'] + \" \" + question_list['format']\n",
    "#             # lowercase the first word, just in case\n",
    "#             question = question.lstrip() # no whitespace\n",
    "#             question = question[0].lower() + question[1:]\n",
    "#             if verbose: print('   on question:',question)\n",
    "#             # Prepare the API request\n",
    "#             prompt = f\"I am going to show you an image. Here is the image: [Image: {encoded_image}]. Now, {question}\"\n",
    "#             prompt_save = f\"I am going to show you an image. Here is the image: [Image: <ENCODED IMAGE>]. Now, {question}\"\n",
    "#             ##question_list['prompt'] = prompt\n",
    "            \n",
    "#             if not test_run:\n",
    "#                 # Send the request to the Claude api\n",
    "#                 response = client.messages.create(\n",
    "#                     model = model,\n",
    "#                     max_tokens=max_tokens,\n",
    "#                     system = system_prompt,\n",
    "#                     temperature=temperature,\n",
    "#                     messages=[\n",
    "#                         {\n",
    "#                             \"role\": \"user\",\n",
    "#                             \"content\": [\n",
    "#                                 {\n",
    "#                                     \"type\": \"image\",\n",
    "#                                     \"source\": {\n",
    "#                                         \"type\": \"base64\",\n",
    "#                                         \"media_type\": media_type,\n",
    "#                                         \"data\": encoded_image,\n",
    "#                                     },\n",
    "#                                 },\n",
    "#                                 {\n",
    "#                                     \"type\": \"text\",\n",
    "#                                     \"text\": prompt\n",
    "#                                 }\n",
    "#                             ],\n",
    "#                         }\n",
    "#                         # {\"role\": \"system\", \"content\": question_list['persona']},\n",
    "#                         # {\"role\":\"user\", \"content\": [\n",
    "#                         #     {\n",
    "#                         #     \"type\": \"text\",\n",
    "#                         #     \"text\": prompt\n",
    "#                         #     },\n",
    "#                         #     {\n",
    "#                         #     \"type\": \"image_url\",\n",
    "#                         #     \"image_url\": {\n",
    "#                         #         \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "#                         #     }\n",
    "#                         #     }\n",
    "#                         # ]\n",
    "#                         # }\n",
    "#                     ]\n",
    "#                 )\n",
    "#                 success = True\n",
    "#             else:\n",
    "#                 success = True\n",
    "#         except RateLimitError as e:\n",
    "#             if attempt < max_retries - 1:\n",
    "#                 # Exponential backoff with jitter\n",
    "#                 wait_time = sleep_time*(2 ** (attempt)) + np.random.uniform(0, 1)\n",
    "#                 print(f\"      Rate limit hit. Waiting {wait_time:.2f} seconds before retry {attempt + 1}\")\n",
    "#                 time.sleep(wait_time)\n",
    "#                 attempt += 1\n",
    "#             else:\n",
    "#                 print(f\"Max retries exceeded. Error: {e}\")\n",
    "#                 raise\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             new_fac = fac/iFac\n",
    "#             print('      new fac = ', new_fac)\n",
    "#             encoded_image = load_image(image_path,fac=new_fac, tmp_dir=tmp_dir)\n",
    "#             iFac += 1\n",
    "    \n",
    "#     print(response)\n",
    "#     if not test_run:\n",
    "#         # Get the response from the API\n",
    "#         answer = response.content[0].text #response.choices[0].message.content\n",
    "#         question_list['raw answer'] = answer\n",
    "#         # also calculate usage\n",
    "#         usage = response.usage\n",
    "#         question_list['usage'] = usage\n",
    "#         if verbose:\n",
    "#             print(f\"Input tokens: {usage.input_tokens}\")\n",
    "#             print(f\"Output tokens: {usage.output_tokens}\")\n",
    "#             print(f\"Total tokens: {usage.input_tokens + usage.output_tokens}\")\n",
    "#         # format answer\n",
    "#         answer_format = answer.split('```json\"')[-1].split('\\n')[0].replace('\\n', '')\n",
    "#         #answer.replace(\"```json\\n\",'').replace(\"\\n```\",'')\n",
    "#         try:\n",
    "#             question_list['Response'] = json.loads(answer_format)\n",
    "#         except:\n",
    "#             question_list['Response'] = answer_format\n",
    "#             question_list['Error'] = 'JSON formatting'\n",
    "#         question_list['Response String'] = answer_format\n",
    "#         success = True\n",
    "#     else:\n",
    "#         question_list['Response'] = 'TEST RUN'\n",
    "#         question_list['Response String'] = 'TEST RUN'\n",
    "\n",
    "#     return question_list, prompt_save, system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "5785e760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_for_errors_claude(qa_in, verbose=True):\n",
    "    # there is some \"doubling up\" of strings, so clean up a bit\n",
    "    direct_copy_list = ['Q', 'A', 'Level', 'type', 'persona', \n",
    "                        'context', 'question', 'format', 'plot number', \n",
    "                        'usage', 'prompt', 'system prompt',\n",
    "                        'Response', 'Response String'] # these last 2 will be overwritten\n",
    "    qa_out = []\n",
    "    for qa_pairs in qa_in:\n",
    "        if verbose:\n",
    "            print('Prompt:', qa_pairs['prompt'])\n",
    "            print('  Real A:', qa_pairs['A'])\n",
    "        #response = qa_pairs['Response'].split('```json')\n",
    "        response_claude_raw = qa_pairs['raw answer']\n",
    "        response_claude = ''\n",
    "        try:\n",
    "            if '```json' in response_claude_raw: # ideal\n",
    "                response_claude = response_claude_raw.split('```json')[-1].split('```')[0].replace('\\n','')\n",
    "                response_claude = json.loads(response_claude)\n",
    "            elif '{\"' in response_claude_raw: # less ideal\n",
    "                response_claude = '{\"' + response_claude_raw.split('{\"')[-1].replace('\\n','')\n",
    "                response_claude = json.loads(response_claude)\n",
    "        except json.JSONDecodeError: # last ditch effort\n",
    "            # Extract JSON if there's extra text\n",
    "            json_match = re.search(r'\\{.*\\}', response_claude_raw, re.DOTALL)\n",
    "            if json_match:\n",
    "                try:\n",
    "                    response_claude = json.loads(json_match.group())\n",
    "                except json.JSONDecodeError:\n",
    "                    pass\n",
    "        if verbose:\n",
    "            if response_claude == '':\n",
    "                print(\"ERROR IN CLAUDE PARSE\")\n",
    "            else:\n",
    "                print('Claude A:', response_claude)\n",
    "            print('')\n",
    "        # now clean up\n",
    "        qa_dir = {}\n",
    "        for dc in direct_copy_list:\n",
    "            qa_dir[dc] = qa_pairs[dc]\n",
    "        # overwrite the last two\n",
    "        qa_dir['Response String'] = qa_pairs['raw answer'] # full, un filtered answer\n",
    "        qa_dir['Response'] = response_claude\n",
    "        qa_out.append(qa_dir.copy())\n",
    "    return qa_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "de214bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on 0 of 2\n",
      "have file already: /Users/jnaiman/LLM_VQA_JCDL2025/example_hists/LLM_outputs/claude_api/nclust_3_trial9.pickle\n",
      "on 1 of 2\n",
      "   on question: how many bars are there in the specified figure panel? Please format the output as a json as {\"nbars\":\"\"} for this figure panel, where the \"nbars\" value should be an integer.\n",
      "      - Input tokens: 534\n",
      "      - Output tokens: 15\n",
      "      - Total tokens: 549\n",
      "   on question: what are the maximum data values in this figure panel?  Please format the output as a json as {\"maximum x\":\"\"} for this figure panel, where the \"maximum\" value should be a float, calculated from the data values used to create the plot.\n",
      "      - Input tokens: 545\n",
      "      - Output tokens: 17\n",
      "      - Total tokens: 562\n",
      "   on question: what are the mean data values in this figure panel?  Please format the output as a json as {\"mean x\":\"\"} for this figure panel, where the \"mean\" value should be a float, calculated from the data values used to create the plot.\n",
      "      - Input tokens: 545\n",
      "      - Output tokens: 17\n",
      "      - Total tokens: 562\n",
      "   on question: what are the median data values in this figure panel?  Please format the output as a json as {\"median x\":\"\"} for this figure panel, where the \"median\" value should be a float, calculated from the data values used to create the plot.\n",
      "      - Input tokens: 545\n",
      "      - Output tokens: 17\n",
      "      - Total tokens: 562\n",
      "   on question: what are the minimum data values in this figure panel?  Please format the output as a json as {\"minimum x\":\"\"} for this figure panel, where the \"minimum\" value should be a float, calculated from the data values used to create the plot.\n",
      "      - Input tokens: 545\n",
      "      - Output tokens: 12\n",
      "      - Total tokens: 557\n",
      "   on question: how many gaussians were used to generate the data for the plot in the figure panel? Please format the output as a json as {\"ngaussians\":\"\"} for this figure panel, where the \"ngaussians\" value should be an integer.\n",
      "      - Input tokens: 543\n",
      "      - Output tokens: 16\n",
      "      - Total tokens: 559\n",
      "\n",
      "**** Cleaned QA ****\n",
      "Prompt: I am going to show you an image. Now, how many bars are there in the specified figure panel? Please format the output as a json as {\"nbars\":\"\"} for this figure panel, where the \"nbars\" value should be an integer.\n",
      "  Real A: 50\n",
      "Claude A: {'nbars': 100}\n",
      "\n",
      "Prompt: I am going to show you an image. Now, what are the maximum data values in this figure panel?  Please format the output as a json as {\"maximum x\":\"\"} for this figure panel, where the \"maximum\" value should be a float, calculated from the data values used to create the plot.\n",
      "  Real A: 0.531999058363311\n",
      "Claude A: {'maximum x': 0.5}\n",
      "\n",
      "Prompt: I am going to show you an image. Now, what are the mean data values in this figure panel?  Please format the output as a json as {\"mean x\":\"\"} for this figure panel, where the \"mean\" value should be a float, calculated from the data values used to create the plot.\n",
      "  Real A: -0.03567869693057275\n",
      "Claude A: {'mean x': 0.0}\n",
      "\n",
      "Prompt: I am going to show you an image. Now, what are the median data values in this figure panel?  Please format the output as a json as {\"median x\":\"\"} for this figure panel, where the \"median\" value should be a float, calculated from the data values used to create the plot.\n",
      "  Real A: -0.11391004457919605\n",
      "Claude A: {'median x': '0.0'}\n",
      "\n",
      "Prompt: I am going to show you an image. Now, what are the minimum data values in this figure panel?  Please format the output as a json as {\"minimum x\":\"\"} for this figure panel, where the \"minimum\" value should be a float, calculated from the data values used to create the plot.\n",
      "  Real A: -0.44113410366866\n",
      "Claude A: {'minimum x': -0.5}\n",
      "\n",
      "Prompt: I am going to show you an image. Now, how many gaussians were used to generate the data for the plot in the figure panel? Please format the output as a json as {\"ngaussians\":\"\"} for this figure panel, where the \"ngaussians\" value should be an integer.\n",
      "  Real A: 5\n",
      "Claude A: {'ngaussians': '3'}\n",
      "\n",
      "Just saved: /Users/jnaiman/LLM_VQA_JCDL2025/example_hists/LLM_outputs/claude_api/nclust_5_trial3.pickle\n"
     ]
    }
   ],
   "source": [
    "iMax = 2\n",
    "verbose = False\n",
    "test_run = False # run w/o actually pinging openai\n",
    "restart = False\n",
    "model =\"claude-sonnet-4-20250514\"\n",
    "max_tokens=500\n",
    "# set system_prompt to None to default to what is in question list\n",
    "system_prompt = \"\"\"You are a helpful assistant that responds only in valid JSON format. Do not include any explanations, reasoning, or text outside of the JSON response.\"\"\"\n",
    "#system_prompt = \"\"\"You must respond with only valid JSON. Start your response immediately with { and end with }. Do not write any text before or after the JSON.\"\"\"\n",
    "temperature=0.1\n",
    "fac = 1.0\n",
    "sleep = 5 # seconds\n",
    "use_single_prompt = True # use 1 prompt and 1 image, if False will use multiple at a time\n",
    "\n",
    "\n",
    "for ijson,json_path in enumerate(jsons_to_parse):\n",
    "    if ijson >= iMax:\n",
    "        continue\n",
    "\n",
    "    print('on', ijson, 'of', iMax)\n",
    "\n",
    "    # get image and base json\n",
    "    img_path = imgs_dir + json_path.split('/')[-1].removesuffix('.json') + '.' + img_format\n",
    "    encoded_image, img_format_media, base_json, err = get_img_json_pair(img_path, json_path, dir_api, \n",
    "                                                      fac=fac, restart=restart,\n",
    "                                                      tmp_dir=tmp_dir)\n",
    "    if err:\n",
    "        continue\n",
    "\n",
    "    ###### create QA ########\n",
    "    qa = []\n",
    "    \n",
    "    for k,v in base_json['VQA']['Level 1']['Figure-level questions'].items():\n",
    "        out = {'Q':v['Q'], 'A':v['A'], 'Level':'Level 1', 'type':'Figure-level questions', 'Response':\"\"}\n",
    "        qa.append(out)\n",
    "    \n",
    "    # what kinds?\n",
    "    types = ['(words + list)', '(words)']\n",
    "    \n",
    "    # get uniques\n",
    "    level_parse = 'Level 1'\n",
    "    plot_level = 'Plot-level questions'\n",
    "    qa = parse_qa(level_parse, plot_level, qa, base_json['VQA'], types)\n",
    "    \n",
    "    level_parse = 'Level 2'\n",
    "    plot_level = 'Plot-level questions'\n",
    "    qa = parse_qa(level_parse, plot_level, qa, base_json['VQA'], types)\n",
    "    \n",
    "    level_parse = 'Level 3'\n",
    "    plot_level = 'Plot-level questions'\n",
    "    qa = parse_qa(level_parse, plot_level, qa, base_json['VQA'], types)\n",
    "\n",
    "    responses = []; prompts = []; system_prompts = []\n",
    "    if use_single_prompt:\n",
    "        for question_list in qa:\n",
    "            response, prompt, system_prompt_out = send_to_claude(question_list, client, img_path, encoded_image,\n",
    "                        model = model, max_tokens=max_tokens, media_type='image/' + img_format_media,\n",
    "                        test_run = test_run, system_prompt=system_prompt, temperature=temperature, \n",
    "                        sleep_time=sleep)\n",
    "            responses.append(response)\n",
    "            question_list['prompt'] = prompt\n",
    "            question_list['system prompt'] = system_prompt_out\n",
    "        #import sys; sys.exit() # just do one\n",
    "        time.sleep(sleep)\n",
    "    time.sleep(sleep)\n",
    "\n",
    "    # parse for errors\n",
    "    #qa = parse_for_errors(qa, llm='claude')\n",
    "    print('')\n",
    "    print('**** Cleaned QA ****')\n",
    "    qa = parse_for_errors_claude(qa)\n",
    "\n",
    "    # dump to file\n",
    "    if not test_run:\n",
    "        with open(dir_api + json_path.split('/')[-1].removesuffix('.json')+ '.pickle', 'wb') as ff:\n",
    "            pickle.dump([qa, model], ff)\n",
    "        print('Just saved:', dir_api + json_path.split('/')[-1].removesuffix('.json')+ '.pickle')\n",
    "    else:\n",
    "        print('Would store at:', dir_api + json_path.split('/')[-1].removesuffix('.json')+ '.pickle')\n",
    "    #import sys; sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b90896",
   "metadata": {},
   "source": [
    "## Look at data\n",
    "\n",
    "Check out one, if you wanna:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f8540406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/LLM_outputs/claude_api/nclust_3_trial9.pickle']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pickles = glob(dir_api + '*.pickle')\n",
    "#pickles = glob('/Users/jnaiman/Downloads/tmp/JCDL2025/example_hists/claude_api/*pickle')\n",
    "pickles[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bc36f1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ifile = 0\n",
    "with open(pickles[ifile], 'rb') as f:\n",
    "    qa_in = pickle.load(f)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dc0a5ebf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Q': 'You are a helpful assistant that can analyze images.  How many bars are there in the specified figure panel? Please format the output as a json as {\"nbars\":\"\"} for this figure panel, where the \"nbars\" value should be an integer.',\n",
       " 'A': 50,\n",
       " 'Level': 'Level 1',\n",
       " 'type': 'Plot-level questions',\n",
       " 'persona': 'You are a helpful assistant that can analyze images.',\n",
       " 'context': '',\n",
       " 'question': 'How many bars are there in the specified figure panel?',\n",
       " 'format': 'Please format the output as a json as {\"nbars\":\"\"} for this figure panel, where the \"nbars\" value should be an integer.',\n",
       " 'plot number': 'plot0',\n",
       " 'usage': Usage(cache_creation_input_tokens=0, cache_read_input_tokens=0, input_tokens=534, output_tokens=15, server_tool_use=None, service_tier='standard'),\n",
       " 'prompt': 'I am going to show you an image. Now, how many bars are there in the specified figure panel? Please format the output as a json as {\"nbars\":\"\"} for this figure panel, where the \"nbars\" value should be an integer.',\n",
       " 'system prompt': 'You are a helpful assistant that responds only in valid JSON format. Do not include any explanations, reasoning, or text outside of the JSON response.',\n",
       " 'Response': {'nbars': 100},\n",
       " 'Response String': '```json\\n{\"nbars\": 100}\\n```'}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_in[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b63377f",
   "metadata": {},
   "source": [
    "Claude outputs reasoning, so we have to do a bit of cleaning from the responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4e69ab74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/jnaiman/LLM_VQA_JCDL2025/example_hists/LLM_outputs/claude_api/nclust_3_trial9.pickle\n",
      "*********\n",
      "Prompt: I am going to show you an image. Now, how many bars are there in the specified figure panel? Please format the output as a json as {\"nbars\":\"\"} for this figure panel, where the \"nbars\" value should be an integer.\n",
      "  Real A: 50\n",
      "Claude A: {'nbars': 100}\n",
      "\n",
      "Prompt: I am going to show you an image. Now, what are the maximum data values in this figure panel?  Please format the output as a json as {\"maximum x\":\"\"} for this figure panel, where the \"maximum\" value should be a float, calculated from the data values used to create the plot.\n",
      "  Real A: 0.49755257997301794\n",
      "Claude A: {'maximum x': 0.5}\n",
      "\n",
      "Prompt: I am going to show you an image. Now, what are the mean data values in this figure panel?  Please format the output as a json as {\"mean x\":\"\"} for this figure panel, where the \"mean\" value should be a float, calculated from the data values used to create the plot.\n",
      "  Real A: 0.34264837991488106\n",
      "Claude A: {'mean x': 0.3}\n",
      "\n",
      "Prompt: I am going to show you an image. Now, what are the median data values in this figure panel?  Please format the output as a json as {\"median x\":\"\"} for this figure panel, where the \"median\" value should be a float, calculated from the data values used to create the plot.\n",
      "  Real A: 0.3895267656798662\n",
      "Claude A: {'median x': '0.3'}\n",
      "\n",
      "Prompt: I am going to show you an image. Now, what are the minimum data values in this figure panel?  Please format the output as a json as {\"minimum x\":\"\"} for this figure panel, where the \"minimum\" value should be a float, calculated from the data values used to create the plot.\n",
      "  Real A: 0.15863689997095148\n",
      "Claude A: {'minimum x': 0.1}\n",
      "\n",
      "Prompt: I am going to show you an image. Now, how many gaussians were used to generate the data for the plot in the figure panel? Please format the output as a json as {\"ngaussians\":\"\"} for this figure panel, where the \"ngaussians\" value should be an integer.\n",
      "  Real A: 3\n",
      "Claude A: {'ngaussians': '2'}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(pickles[ifile])\n",
    "print('*********')\n",
    "for qa_pairs in qa_in:\n",
    "    print('Prompt:', qa_pairs['prompt'])\n",
    "    print('  Real A:', qa_pairs['A'])\n",
    "    print('Claude A:', qa_pairs['Response'])\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57affca7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JCDL2025",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
